{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb49d163-3305-4bd7-be6a-2dbf0c186f8d",
   "metadata": {},
   "source": [
    "# Siamese Neural Networks for Movie Trailer Recommendation System\n",
    "\n",
    "### MASTER TRAINER\n",
    "* able to run all 4 models (2x2)\n",
    "* cosine vs euclidean\n",
    "* pair vs triplet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c3b0fc-52ce-4879-b768-31639387c017",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### code prepration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67184f37-0f21-4651-b717-060681577647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RUN AND THEN RESTART KERNEL!\n",
    "# import subprocess\n",
    "# import sys\n",
    "\n",
    "# # Downgrade protobuf to compatible version\n",
    "# result = subprocess.run([sys.executable, '-m', 'pip', 'install', 'protobuf==3.20.3'], \n",
    "#                        capture_output=True, text=True)\n",
    "# print(\"STDOUT:\", result.stdout)\n",
    "# print(\"STDERR:\", result.stderr)\n",
    "\n",
    "# !export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa7c9c2d-e929-4a55-a7e1-dddcb122aef2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install torchvision transformers\n",
    "# !pip install torch torchvision transformers numpy pandas scikit-learn tqdm librosa opencv-python matplotlib tensorboard boto3 av \n",
    "# !pip install resampy soundfile\n",
    "# !pip install ffmpeg-python\n",
    "# !pip install easyocr\n",
    "# !pip install torchinfo\n",
    "# !pip install kornia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "502b313b-1308-4108-88e1-ac53c568270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge ffmpeg -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "928c4f0a-3976-438f-9c28-87086b05106f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f257d9bf100>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Standard Library Imports\n",
    "import gc\n",
    "import io\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import tempfile\n",
    "import time\n",
    "import traceback\n",
    "import warnings\n",
    "from collections import defaultdict, OrderedDict\n",
    "from datetime import datetime\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party Core Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "from tqdm import tqdm\n",
    "# from tqdm.auto import tqdm  # For notebook compatibility\n",
    "\n",
    "# Machine Learning & Scientific Computing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score, confusion_matrix,\n",
    "    precision_recall_curve\n",
    ")\n",
    "\n",
    "# Deep Learning - PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils as utils\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchinfo import summary\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "# Deep Learning - Transformers & Specialized\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import kornia.filters as KFilters\n",
    "\n",
    "# Computer Vision & Audio Processing\n",
    "import cv2\n",
    "import librosa\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Cloud & External Services\n",
    "import boto3\n",
    "\n",
    "# Monitoring & Logging\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "\n",
    "# Warning Filters Configuration\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"librosa\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"PySoundFile failed.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*Parameter.*\")\n",
    "cv2.setLogLevel(0)  # Suppress OpenCV warnings\n",
    "\n",
    "# Logging Configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# CUDA Configuration\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# Reproducibility Setup\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set seeds for reproducible results across all libraries.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print('All imports successfully loaded! Ready to go!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050a217c-7567-43a5-ab64-648ce02b7839",
   "metadata": {},
   "source": [
    "## Feature Extractor Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048ef6f7-d0e4-492d-9980-f7801d518796",
   "metadata": {},
   "source": [
    "### Visual Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b87724-d69f-4676-bf60-92768c5c19e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualProcessingModule(nn.Module):\n",
    "    def __init__(self, backbone=\"dinov2\", low_level_features=True, use_precomputed_embeddings=False, dinov2_embedding_dim=384):\n",
    "        \"\"\"\n",
    "        Visual Processing Module that can work with either:\n",
    "        1. Raw frames + backbone processing (original mode)\n",
    "        2. Precomputed DINOv2 embeddings (new mode)\n",
    "        \n",
    "        Args:\n",
    "            backbone: \"dinov2\" or \"resnet\" - only used when use_precomputed_embeddings=False\n",
    "            low_level_features: Whether to extract and use low-level visual features\n",
    "            use_precomputed_embeddings: If True, expects precomputed DINOv2 embeddings as input\n",
    "            dinov2_embedding_dim: Dimension of DINOv2 embeddings (384 for ViT-S/14)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.backbone_type = backbone\n",
    "        self.low_level_features = low_level_features\n",
    "        self.use_precomputed_embeddings = use_precomputed_embeddings\n",
    "\n",
    "        # Setup backbone and feature dimensions\n",
    "        if use_precomputed_embeddings:\n",
    "            # No backbone needed, we'll receive precomputed embeddings\n",
    "            self.backbone = None\n",
    "            self.feature_dim = dinov2_embedding_dim\n",
    "            print(f\"Using precomputed embeddings mode with feature_dim={self.feature_dim}\")\n",
    "        else:\n",
    "            # Original mode - setup backbone\n",
    "            if backbone == \"dinov2\":\n",
    "                self.backbone = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14', trust_repo=True)\n",
    "                self.feature_dim = 384\n",
    "                \n",
    "                # Conservative fine-tuning: Only unfreeze the final norm layer\n",
    "                print(\"DINOv2 Fine-tuning: Conservative - ONLY final 'norm' layer.\")\n",
    "                for name, param in self.backbone.named_parameters():\n",
    "                    param.requires_grad = False  # Freeze all first\n",
    "                    \n",
    "                if hasattr(self.backbone, 'norm') and isinstance(self.backbone.norm, nn.LayerNorm):\n",
    "                    print(\"Unfreezing DINOv2's final 'norm' layer.\")\n",
    "                    for param in self.backbone.norm.parameters():\n",
    "                        param.requires_grad = True\n",
    "                    \n",
    "                    num_trainable = sum(p.numel() for p in self.backbone.parameters() if p.requires_grad)\n",
    "                    print(f\"DINOv2 trainable parameters: {num_trainable}\")\n",
    "                else:\n",
    "                    print(\"DINOv2 'norm' layer not found. Backbone remains fully frozen.\")\n",
    "                    \n",
    "            elif backbone == \"resnet\":\n",
    "                self.backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "                self.backbone = nn.Sequential(*list(self.backbone.children())[:-1])\n",
    "                self.feature_dim = 2048\n",
    "                \n",
    "                # Freeze all but last few layers\n",
    "                for param in list(self.backbone.parameters())[:-10]:\n",
    "                    param.requires_grad = False\n",
    "\n",
    "        # MLP layers for processing features\n",
    "        self.norm_after_backbone_pooling = nn.LayerNorm(self.feature_dim, eps=1e-5)\n",
    "        self.dropout_before_fusion = nn.Dropout(0.4)\n",
    "        \n",
    "        self.frame_fusion_mlp = nn.Sequential(\n",
    "            nn.Linear(self.feature_dim, self.feature_dim // 2),\n",
    "            nn.BatchNorm1d(self.feature_dim // 2, eps=1e-5),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(self.feature_dim // 2, self.feature_dim // 2)\n",
    "        )\n",
    "        \n",
    "        self.dropout_after_fusion = nn.Dropout(0.4)\n",
    "\n",
    "        # Low-level feature processing\n",
    "        if low_level_features:\n",
    "            self.low_level_dim_output = 64\n",
    "            self.low_level_projection = nn.Linear(5, self.low_level_dim_output)\n",
    "            self.output_dim = (self.feature_dim // 2) + self.low_level_dim_output\n",
    "        else:\n",
    "            self.output_dim = self.feature_dim // 2\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with conservative gains to prevent gradient explosion.\"\"\"\n",
    "        for module_name, module in self.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                if 'frame_fusion_mlp.0' in module_name:\n",
    "                    gain = 1.0\n",
    "                    print(f\"Applying standard gain ({gain}) to {module_name}\")\n",
    "                    nn.init.xavier_uniform_(module.weight, gain=gain)\n",
    "                else:\n",
    "                    gain = nn.init.calculate_gain('relu')\n",
    "                    nn.init.xavier_uniform_(module.weight, gain=gain)\n",
    "                \n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0.0)\n",
    "                    \n",
    "            elif isinstance(module, (nn.LayerNorm, nn.BatchNorm1d)):\n",
    "                nn.init.constant_(module.weight, 1.0)\n",
    "                nn.init.constant_(module.bias, 0.0)\n",
    "\n",
    "    def forward(self, frames, precomputed_dino_embeddings=None):\n",
    "        \"\"\"\n",
    "        Forward pass supporting both modes:\n",
    "        \n",
    "        Mode 1 (original): forward(frames)\n",
    "        - frames: (Batch, NumFrames, C, H, W)\n",
    "        \n",
    "        Mode 2 (precomputed): forward(frames, precomputed_dino_embeddings)\n",
    "        - frames: (Batch, NumFrames, C, H, W) - used only for low-level features\n",
    "        - precomputed_dino_embeddings: (Batch, NumFrames, DINO_DIM)\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.use_precomputed_embeddings:\n",
    "            if precomputed_dino_embeddings is None:\n",
    "                raise ValueError(\"precomputed_dino_embeddings must be provided when use_precomputed_embeddings=True\")\n",
    "            return self._forward_with_precomputed(frames, precomputed_dino_embeddings)\n",
    "        else:\n",
    "            return self._forward_with_backbone(frames)\n",
    "\n",
    "    def _forward_with_precomputed(self, frames_for_low_level, precomputed_dino_frame_embeddings):\n",
    "        \"\"\"Forward pass using precomputed DINOv2 embeddings.\"\"\"\n",
    "        \n",
    "        # 1. Pool the precomputed per-frame embeddings\n",
    "        video_features_pooled = precomputed_dino_frame_embeddings.mean(dim=1)  # (Batch, DINO_DIM)\n",
    "        \n",
    "        # --- ADDED L2 NORMALIZATION FOR POOLED DINO FEATURES ---\n",
    "        if torch.isnan(video_features_pooled).any() or torch.isinf(video_features_pooled).any():\n",
    "            print(\"V- Pooled DINO had NaN/Inf before L2 norm. Clamping.\")\n",
    "            video_features_pooled = torch.nan_to_num(video_features_pooled, nan=0.0, posinf=1.0, neginf=-1.0) # Added clamping range\n",
    "        \n",
    "        # Stabilize before F.normalize\n",
    "        pooled_dino_norm_val = torch.norm(video_features_pooled, p=2, dim=1, keepdim=True)\n",
    "        # Add a small epsilon to prevent division by zero if norm is exactly zero after nan_to_num\n",
    "        if torch.any(pooled_dino_norm_val < 1e-7): \n",
    "            noise = torch.randn_like(video_features_pooled) * 1e-7\n",
    "            # Apply noise only where norm is too small\n",
    "            video_features_pooled = torch.where(\n",
    "                (pooled_dino_norm_val < 1e-7).expand_as(video_features_pooled), \n",
    "                video_features_pooled + noise, \n",
    "                video_features_pooled\n",
    "            )\n",
    "            \n",
    "        video_features_pooled_for_norm = video_features_pooled\n",
    "        if torch.any(torch.norm(video_features_pooled_for_norm, p=2, dim=1) < 1e-7): # Check again\n",
    "             # If still zero norm for some (e.g. all-zero input + all-zero noise), make them tiny non-zero\n",
    "             video_features_pooled_for_norm = video_features_pooled_for_norm + 1e-7 * torch.ones_like(video_features_pooled_for_norm)\n",
    "\n",
    "\n",
    "        video_features_pooled_normalized_l2 = F.normalize(video_features_pooled_for_norm, p=2, dim=1, eps=1e-6)\n",
    "\n",
    "        # 2. Normalization and MLP processing\n",
    "        video_features_normed_by_layernorm = self.norm_after_backbone_pooling(video_features_pooled_normalized_l2) # Pass the L2-normalized version\n",
    "        \n",
    "        if torch.isnan(video_features_normed_by_layernorm).any() or torch.isinf(video_features_normed_by_layernorm).any():\n",
    "            video_features_normed_by_layernorm = torch.nan_to_num(video_features_normed_by_layernorm, nan=0.0)\n",
    "\n",
    "        video_features_to_fuse = self.dropout_before_fusion(video_features_normed_by_layernorm)\n",
    "        \n",
    "        # Check BatchNorm variance if needed\n",
    "        if video_features_to_fuse.size(0) > 1: # Ensure batch size > 1 for variance calculation\n",
    "            input_to_bn = self.frame_fusion_mlp[0](video_features_to_fuse)\n",
    "            # Ensure input_to_bn is not empty and has variance before checking\n",
    "            if input_to_bn.numel() > 0 and input_to_bn.size(0) > 1: \n",
    "                var_bn_input_check = input_to_bn.var(dim=0, unbiased=False)\n",
    "                if (var_bn_input_check < 1e-6).any():\n",
    "                    problematic_channels = (var_bn_input_check < 1e-6).sum().item()\n",
    "\n",
    "        fused_video_features = self.frame_fusion_mlp(video_features_to_fuse)\n",
    "        if torch.isnan(fused_video_features).any() or torch.isinf(fused_video_features).any():\n",
    "            fused_video_features = torch.nan_to_num(fused_video_features, nan=0.0)\n",
    "\n",
    "        final_high_level_features = self.dropout_after_fusion(fused_video_features)\n",
    "\n",
    "        # 3. Low-level features (calculated on-the-fly using original frames)\n",
    "        if self.low_level_features and frames_for_low_level is not None:\n",
    "            low_level_raw_avg = self.extract_low_level_features(frames_for_low_level)  # (Batch, 5)\n",
    "            \n",
    "            if torch.isnan(low_level_raw_avg).any():\n",
    "                low_level_raw_avg = torch.nan_to_num(low_level_raw_avg, nan=0.0)\n",
    "            \n",
    "            low_level_projected = self.low_level_projection(low_level_raw_avg)\n",
    "            low_level_projected = torch.tanh(low_level_projected)\n",
    "            if torch.isnan(low_level_projected).any():\n",
    "                low_level_projected = torch.nan_to_num(low_level_projected, nan=0.0)\n",
    "\n",
    "            output_features = torch.cat([final_high_level_features, low_level_projected], dim=1)\n",
    "        else:\n",
    "            output_features = final_high_level_features\n",
    "        \n",
    "        if torch.isnan(output_features).any():\n",
    "            output_features = torch.nan_to_num(output_features, nan=0.0)\n",
    "        \n",
    "        output_magnitude_visual = torch.norm(output_features, p=2, dim=1, keepdim=True)\n",
    "        if torch.any(output_magnitude_visual < 1e-7):\n",
    "            noise_visual = torch.randn_like(output_features) * 1e-7\n",
    "            output_features = torch.where((output_magnitude_visual < 1e-7).expand_as(output_features), output_features + noise_visual, output_features)\n",
    "            # print(f\"V- Applied noise to final visual features with near-zero norm before F.normalize.\")\n",
    "\n",
    "        output_features_final_normalized = F.normalize(output_features, p=2, dim=1, eps=1e-6)\n",
    "        \n",
    "        # print(f\"V- Post-FinalNorm: mean={output_features_final_normalized.mean().item():.4f}, std={output_features_final_normalized.std().item():.4f}, norm={torch.norm(output_features_final_normalized, p=2, dim=1).mean().item():.4f}\")\n",
    "\n",
    "        if torch.isnan(output_features_final_normalized).any():\n",
    "            output_features_final_normalized = torch.zeros_like(output_features_final_normalized)\n",
    "            \n",
    "        return output_features_final_normalized\n",
    "\n",
    "    def _forward_with_backbone(self, frames):\n",
    "        \"\"\"Original forward pass using backbone feature extraction.\"\"\"\n",
    "        batch_size, num_frames = frames.shape[0], frames.shape[1]\n",
    "\n",
    "        if torch.isnan(frames).any():\n",
    "            print(\"V- WARNING: Input frames contain NaN values! Replacing with zeros.\")\n",
    "            frames = torch.nan_to_num(frames, nan=0.0)\n",
    "\n",
    "        # 1. Backbone Feature Extraction\n",
    "        # Handle partial freezing for DINOv2\n",
    "        if self.backbone_type == \"dinov2\":\n",
    "            # Store original requires_grad states\n",
    "            original_requires_grad = {}\n",
    "            for name, param in self.backbone.named_parameters():\n",
    "                original_requires_grad[name] = param.requires_grad\n",
    "                # Temporarily freeze parts that should be frozen during forward pass\n",
    "                if 'norm' not in name:\n",
    "                    param.requires_grad_(False)\n",
    "        \n",
    "        frames_flat = frames.view(-1, 3, frames.shape[3], frames.shape[4])\n",
    "        chunk_size = 8  # Adjust based on GPU memory\n",
    "        frame_features_list = []\n",
    "        \n",
    "        for i in range(0, frames_flat.size(0), chunk_size):\n",
    "            chunk = frames_flat[i:i+chunk_size]\n",
    "            chunk_features = self.backbone(chunk)\n",
    "            frame_features_list.append(chunk_features)\n",
    "        \n",
    "        frame_features_flat = torch.cat(frame_features_list, dim=0)\n",
    "\n",
    "        # Restore original requires_grad states for DINOv2\n",
    "        if self.backbone_type == \"dinov2\":\n",
    "            for name, param in self.backbone.named_parameters():\n",
    "                param.requires_grad_(original_requires_grad[name])\n",
    "\n",
    "        if torch.isnan(frame_features_flat).any() or torch.isinf(frame_features_flat).any():\n",
    "            print(\"V- ❌ CRITICAL: NaN/Inf detected from backbone output! Cleaning.\")\n",
    "            frame_features_flat = torch.nan_to_num(frame_features_flat, nan=0.0, posinf=10.0, neginf=-10.0)\n",
    "        \n",
    "        frame_features = frame_features_flat.view(batch_size, num_frames, -1)\n",
    "        frame_features = torch.clamp(frame_features, -10.0, 10.0)  # Safety clamp\n",
    "\n",
    "        # 2. Temporal Pooling and Normalization\n",
    "        video_features_pooled = frame_features.mean(dim=1)\n",
    "\n",
    "        if torch.isnan(video_features_pooled).any() or torch.isinf(video_features_pooled).any():\n",
    "            print(\"V- ❌ NaN/Inf in video_features_pooled! Zeroing.\")\n",
    "            video_features_pooled = torch.nan_to_num(video_features_pooled, nan=0.0)\n",
    "\n",
    "        video_features_normed = self.norm_after_backbone_pooling(video_features_pooled)\n",
    "        if torch.isnan(video_features_normed).any() or torch.isinf(video_features_normed).any():\n",
    "            print(\"V- ❌ NaN/Inf after normalization! Zeroing.\")\n",
    "            video_features_normed = torch.nan_to_num(video_features_normed, nan=0.0)\n",
    "\n",
    "        # 3. Frame Fusion MLP\n",
    "        video_features_to_fuse = self.dropout_before_fusion(video_features_normed)\n",
    "\n",
    "        fused_video_features = self.frame_fusion_mlp(video_features_to_fuse)\n",
    "\n",
    "        # 4. Final Dropout\n",
    "        final_high_level_features = self.dropout_after_fusion(fused_video_features)\n",
    "\n",
    "        # 5. Low-Level Features (if enabled)\n",
    "        if self.low_level_features:\n",
    "            low_level_raw = self.extract_low_level_features(frames)\n",
    "            if torch.isnan(low_level_raw).any() or torch.isinf(low_level_raw).any():\n",
    "                print(\"V- ❌ NaN/Inf in low_level_raw! Zeroing.\")\n",
    "                low_level_raw = torch.nan_to_num(low_level_raw, nan=0.0)\n",
    "            \n",
    "            low_level_projected = self.low_level_projection(low_level_raw)\n",
    "            low_level_projected = torch.tanh(low_level_projected)  # Bound low-level features\n",
    "            if torch.isnan(low_level_projected).any() or torch.isinf(low_level_projected).any():\n",
    "                print(\"V- ❌ NaN/Inf in low_level_projected! Zeroing.\")\n",
    "                low_level_projected = torch.nan_to_num(low_level_projected, nan=0.0)\n",
    "\n",
    "            output_features = torch.cat([final_high_level_features, low_level_projected], dim=1)\n",
    "        else:\n",
    "            output_features = final_high_level_features\n",
    "        \n",
    "        if torch.isnan(output_features).any() or torch.isinf(output_features).any():\n",
    "            print(\"V- ❌ NaN/Inf in final output_features! Zeroing.\")\n",
    "            output_features = torch.nan_to_num(output_features, nan=0.0)\n",
    "            \n",
    "        return output_features\n",
    "\n",
    "    def extract_low_level_features(self, frames):\n",
    "        \"\"\"Extract low-level visual features like brightness, contrast, edge density.\"\"\"\n",
    "        B, N, C, H, W = frames.shape\n",
    "        frames_flat = frames.view(B * N, C, H, W)\n",
    "\n",
    "        # Convert to grayscale robustly\n",
    "        if frames_flat.dtype == torch.uint8:\n",
    "            frames_flat_float = frames_flat.float() / 255.0\n",
    "        else:\n",
    "            frames_flat_float = frames_flat\n",
    "\n",
    "        gray_frames_flat = (0.299 * frames_flat_float[:, 0:1] + \n",
    "                           0.587 * frames_flat_float[:, 1:2] + \n",
    "                           0.114 * frames_flat_float[:, 2:3])\n",
    "        \n",
    "        brightness = gray_frames_flat.mean(dim=[1, 2, 3])\n",
    "        contrast = gray_frames_flat.std(dim=[1, 2, 3], unbiased=False) + 1e-8\n",
    "\n",
    "        # Edge density using Kornia if available\n",
    "        edge_density = torch.zeros_like(brightness)\n",
    "        if KFilters is not None:\n",
    "            try:\n",
    "                sobel_output = KFilters.sobel(gray_frames_flat)\n",
    "                if sobel_output.shape[1] == 2:  # Returns gx and gy\n",
    "                    sobel_magnitude = torch.sqrt(sobel_output[:,0:1]**2 + sobel_output[:,1:2]**2 + 1e-10)\n",
    "                else:  # Returns magnitude\n",
    "                    sobel_magnitude = sobel_output\n",
    "                edge_density = sobel_magnitude.mean(dim=[1, 2, 3])\n",
    "            except Exception as e:\n",
    "                print(f\"V- Kornia Sobel filter error: {e}. Using zero for edge density.\")\n",
    "                edge_density = torch.zeros_like(brightness)\n",
    "        \n",
    "        # Placeholders for additional features\n",
    "        color_entropy = torch.zeros_like(brightness)\n",
    "        rule_of_thirds_metric = torch.zeros_like(brightness)\n",
    "\n",
    "        low_level_features_flat = torch.stack([\n",
    "            brightness, contrast, edge_density, color_entropy, rule_of_thirds_metric\n",
    "        ], dim=1)\n",
    "        \n",
    "        low_level_features_video = low_level_features_flat.view(B, N, -1)\n",
    "        features_avg = low_level_features_video.mean(dim=1)\n",
    "        \n",
    "        if torch.isnan(features_avg).any():\n",
    "            features_avg = torch.nan_to_num(features_avg, nan=0.0)\n",
    "            \n",
    "        return features_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6fa843-1285-4e48-b4c7-d2d5a550b19a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Audio Processing -optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40966a14-ea1a-4686-b890-a9525caf7997",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedAudioProcessingModule(nn.Module):\n",
    "    def __init__(self, use_vggish=True, use_spectrogram_cnn=True, vggish_embedding_dim=128):\n",
    "        super().__init__()\n",
    "        self.use_vggish = use_vggish\n",
    "        self.use_spectrogram_cnn = use_spectrogram_cnn\n",
    "        \n",
    "        self.vggish_dim = vggish_embedding_dim if self.use_vggish else 0\n",
    "            \n",
    "        # Spectrogram CNN setup\n",
    "        if self.use_spectrogram_cnn:\n",
    "            self.spec_cnn = nn.Sequential(\n",
    "                nn.Conv2d(1, 16, kernel_size=3, padding=1),      # 0\n",
    "                nn.BatchNorm2d(16),                             # 1\n",
    "                nn.GELU(),                                      # 2\n",
    "                nn.MaxPool2d(2),                                # 3\n",
    "                nn.Dropout2d(0.1),                              # 4\n",
    "                nn.Conv2d(16, 32, kernel_size=3, padding=1),    # 5\n",
    "                nn.BatchNorm2d(32),                             # 6\n",
    "                nn.GELU(),                                      # 7\n",
    "                nn.MaxPool2d(2),                                # 8\n",
    "                nn.Dropout2d(0.1),                              # 9\n",
    "                nn.Conv2d(32, 64, kernel_size=3, padding=1),    # 10\n",
    "                nn.BatchNorm2d(64),                             # 11\n",
    "                nn.GELU(),                                      # 12\n",
    "                nn.AdaptiveAvgPool2d((1, 1)),                   # 13\n",
    "                nn.Dropout2d(0.2)                               # 14\n",
    "            )\n",
    "            \n",
    "            self.spec_dim = 64\n",
    "        else:\n",
    "            self.spec_dim = 0\n",
    "\n",
    "        # Fusion layer setup\n",
    "        self.input_to_fusion_dim = self.vggish_dim + self.spec_dim \n",
    "        self.fused_audio_intermediate_dim = 128 \n",
    "        self.final_audio_output_dim = 192 \n",
    "\n",
    "        if self.input_to_fusion_dim > 0 and (self.vggish_dim == 0 or self.spec_dim == 0):\n",
    "            # Only one feature type is active\n",
    "            self.fusion = nn.Identity()\n",
    "            self.output_dim = self.input_to_fusion_dim\n",
    "            print(f\"AUDIO_MODULE: Using Identity for self.fusion as only one audio feature type \"\n",
    "                  f\"({'VGGish' if self.vggish_dim > 0 else 'SpecCNN'}) is active. \"\n",
    "                  f\"Output dim for temporal pooling: {self.output_dim}\")\n",
    "        elif self.vggish_dim > 0 and self.spec_dim > 0:\n",
    "            # Both feature types are active - need fusion\n",
    "            self.fusion = nn.Sequential(\n",
    "                nn.Linear(self.input_to_fusion_dim, self.fused_audio_intermediate_dim),\n",
    "                nn.LayerNorm(self.fused_audio_intermediate_dim, eps=1e-5), \n",
    "                nn.GELU(),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(self.fused_audio_intermediate_dim, self.final_audio_output_dim) \n",
    "            )\n",
    "            self.output_dim = self.final_audio_output_dim \n",
    "        else:\n",
    "            # No audio features active\n",
    "            self.fusion = nn.Identity()\n",
    "            self.output_dim = 0\n",
    "            print(\"AUDIO_MODULE: No audio features active, self.fusion is Identity, output_dim is 0.\")\n",
    "\n",
    "        # Temporal pooling setup\n",
    "        self.temporal_pooling = nn.Sequential(\n",
    "            nn.Linear(self.output_dim if self.output_dim > 0 else 1, self.output_dim if self.output_dim > 0 else 1), \n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        for module_name, module in self.named_modules(): \n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0.0)\n",
    "            elif isinstance(module, nn.Linear):\n",
    "                if 'fusion.' in module_name or 'temporal_pooling.' in module_name:\n",
    "                    gain_val = 0.5 \n",
    "                    nn.init.xavier_uniform_(module.weight, gain=gain_val) \n",
    "                else:\n",
    "                    nn.init.xavier_uniform_(module.weight, gain=0.5) \n",
    "                \n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0.0)\n",
    "            elif isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d, nn.LayerNorm)): \n",
    "                nn.init.constant_(module.weight, 1.0)\n",
    "                nn.init.constant_(module.bias, 0.0)\n",
    "        \n",
    "    def compute_spectrogram_batch(self, audio, sr=16000):\n",
    "        \"\"\"Compute spectrograms for a batch of audio samples.\"\"\"\n",
    "        device = audio.device\n",
    "        batch_size = audio.shape[0]\n",
    "        audio = torch.clamp(audio, -1.0, 1.0)\n",
    "        window = torch.hann_window(512, device=device)\n",
    "        specs = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            audio_sample = audio[i]\n",
    "            if audio_sample.std() > 1e-8: \n",
    "                audio_sample = (audio_sample - audio_sample.mean()) / (audio_sample.std() + 1e-8)\n",
    "            stft = torch.stft(\n",
    "                audio_sample, \n",
    "                n_fft=512, \n",
    "                hop_length=256, \n",
    "                window=window,\n",
    "                return_complex=True\n",
    "            )\n",
    "            spec = torch.abs(stft)\n",
    "            spec = torch.clamp(spec, min=1e-8, max=100.0) \n",
    "            spec = torch.log(spec + 1e-8)\n",
    "            if spec.std() > 1e-8:\n",
    "                spec = (spec - spec.mean()) / (spec.std() + 1e-8)\n",
    "            spec = torch.clamp(spec, -5.0, 5.0) \n",
    "            specs.append(spec)\n",
    "            \n",
    "        specs = torch.stack(specs).unsqueeze(1)\n",
    "        return specs\n",
    "\n",
    "    def forward(self, precomputed_vggish_embedding=None, waveform_for_spec_cnn=None):\n",
    "        \"\"\"\n",
    "        Forward pass with separate inputs for VGGish embeddings and spectrogram CNN.\n",
    "        \n",
    "        Args:\n",
    "            precomputed_vggish_embedding: (Batch, VGGISH_DIM) - precomputed VGGish features\n",
    "            waveform_for_spec_cnn: (Batch, NumSamples) or (Batch, 1, NumSamples) - raw audio for spec CNN\n",
    "        \"\"\"\n",
    "        features_to_combine = []\n",
    "        \n",
    "        # Process VGGish embeddings if provided\n",
    "        if self.use_vggish and precomputed_vggish_embedding is not None:\n",
    "            if torch.isnan(precomputed_vggish_embedding).any() or torch.isinf(precomputed_vggish_embedding).any():\n",
    "                print(\"A- VGGish emb had NaN/Inf before L2 norm. Clamping.\")\n",
    "                precomputed_vggish_embedding = torch.nan_to_num(precomputed_vggish_embedding, nan=0.0)\n",
    "            \n",
    "            # --- ADD L2 NORMALIZATION FOR VGGISH EMBEDDING ---\n",
    "            vggish_norm_val = torch.norm(precomputed_vggish_embedding, p=2, dim=1, keepdim=True)\n",
    "            if torch.any(vggish_norm_val < 1e-7):\n",
    "                noise = torch.randn_like(precomputed_vggish_embedding) * 1e-7\n",
    "                precomputed_vggish_embedding = torch.where((vggish_norm_val < 1e-7).expand_as(precomputed_vggish_embedding),\n",
    "                                                           precomputed_vggish_embedding + noise, precomputed_vggish_embedding)\n",
    "            \n",
    "            normalized_vggish_embedding = F.normalize(precomputed_vggish_embedding, p=2, dim=1, eps=1e-6)\n",
    "            # print(f\"A- VGGish (after L2 norm): mean={normalized_vggish_embedding.mean().item():.4f}, std={normalized_vggish_embedding.std().item():.4f}, norm={torch.norm(normalized_vggish_embedding, p=2, dim=1).mean().item():.4f}\")\n",
    "            features_to_combine.append(normalized_vggish_embedding)\n",
    "            # --- END ADDED L2 NORMALIZATION ---\n",
    "        \n",
    "        # Process spectrogram CNN if waveform provided\n",
    "        if self.use_spectrogram_cnn and waveform_for_spec_cnn is not None:\n",
    "            # Ensure waveform is (Batch, NumSamples) for compute_spectrogram_batch\n",
    "            if waveform_for_spec_cnn.dim() == 3 and waveform_for_spec_cnn.shape[1] == 1:\n",
    "                waveform_for_spec_cnn = waveform_for_spec_cnn.squeeze(1)\n",
    "\n",
    "            specs = self.compute_spectrogram_batch(waveform_for_spec_cnn)  # (Batch, 1, N_MELS, Width)\n",
    "            \n",
    "            # Handle dimension issues\n",
    "            if specs.dim() == 5: \n",
    "                specs = specs.squeeze(2)\n",
    "            specs = torch.clamp(specs, -5.0, 5.0)\n",
    "            \n",
    "            # Process through CNN\n",
    "            spec_cnn_output = self.spec_cnn(specs).squeeze(-1).squeeze(-1)\n",
    "            spec_cnn_output = torch.clamp(spec_cnn_output, -10.0, 10.0)\n",
    "            if torch.isnan(spec_cnn_output).any(): \n",
    "                spec_cnn_output = torch.nan_to_num(spec_cnn_output, nan=0.0)\n",
    "            features_to_combine.append(spec_cnn_output)\n",
    "            \n",
    "        # Handle case where no features are available\n",
    "        if not features_to_combine:\n",
    "            expected_output_dim = self.output_dim if self.output_dim > 0 else 1\n",
    "            # Determine batch size from one of the inputs if possible, or default if all are None\n",
    "            bs = (precomputed_vggish_embedding.shape[0] if precomputed_vggish_embedding is not None else \n",
    "                  (waveform_for_spec_cnn.shape[0] if waveform_for_spec_cnn is not None else 1))\n",
    "            device = (precomputed_vggish_embedding.device if precomputed_vggish_embedding is not None else\n",
    "                     (waveform_for_spec_cnn.device if waveform_for_spec_cnn is not None else torch.device('cpu')))\n",
    "            return torch.zeros(bs, expected_output_dim, device=device)\n",
    "\n",
    "        # Combine features\n",
    "        if len(features_to_combine) > 1:\n",
    "            combined_audio_features = torch.cat(features_to_combine, dim=1)\n",
    "            processed_features = self.fusion(combined_audio_features)\n",
    "        elif features_to_combine:\n",
    "            processed_features = features_to_combine[0]\n",
    "            # Apply fusion if it's not Identity\n",
    "            if not isinstance(self.fusion, nn.Identity):\n",
    "                processed_features = self.fusion(processed_features)\n",
    "\n",
    "        # Apply temporal pooling and normalization\n",
    "        output = self.temporal_pooling(processed_features)\n",
    "        output = F.normalize(output, p=2, dim=1, eps=1e-8)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564198ab-1be8-4ebf-9987-36d0da936d1d",
   "metadata": {},
   "source": [
    "### Text Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e411d67-a9b1-432d-8965-55b5bb208012",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessingModule(nn.Module):\n",
    "    def __init__(self, use_bert=True, use_tfidf=True, num_languages=150,\n",
    "                 lang_embedding_dim=16):\n",
    "        super().__init__()\n",
    "        self.use_bert = use_bert\n",
    "        self.use_tfidf = use_tfidf\n",
    "        \n",
    "        if use_bert:\n",
    "            self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "            self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "            self.bert_dim = 768\n",
    "            for layer in self.bert.encoder.layer:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "            for param in self.bert.pooler.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            self.bert_dim = 0\n",
    "        \n",
    "        if use_tfidf:\n",
    "            self.tfidf_dim = 100\n",
    "            self.tfidf_projection = nn.Linear(5000, self.tfidf_dim)\n",
    "            # Changed eps to be more conservative for LayerNorm\n",
    "            self.tfidf_norm = nn.LayerNorm(self.tfidf_dim, eps=1e-6)\n",
    "        else:\n",
    "            self.tfidf_dim = 0\n",
    "            \n",
    "        # --- NEW: Language and Year Setup ---\n",
    "        self.lang_embedding_dim = lang_embedding_dim\n",
    "        self.language_embedding = nn.Embedding(num_languages, self.lang_embedding_dim)\n",
    "        self.year_dim = 1 # Year is a single feature\n",
    "            \n",
    "        # --- MODIFIED: Update the fusion input dimension ---\n",
    "        self.input_dim = self.bert_dim + self.tfidf_dim + self.lang_embedding_dim + self.year_dim\n",
    "        \n",
    "        # Fusion MLP\n",
    "        self.output_dim = 384\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 256),\n",
    "            nn.BatchNorm1d(256, eps=1e-5),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, self.output_dim)\n",
    "        )\n",
    "        \n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight, gain=1.0)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0.0)\n",
    "            elif isinstance(module, (nn.LayerNorm, nn.BatchNorm1d)):\n",
    "                nn.init.constant_(module.weight, 1.0)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0.0)\n",
    "        \n",
    "    def process_bert(self, text_batch):\n",
    "        encoded = self.tokenizer(text_batch, padding=True, truncation=True, \n",
    "                                max_length=128, return_tensors='pt')\n",
    "        encoded = {k: v.to(next(self.bert.parameters()).device) for k, v in encoded.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert(**encoded)\n",
    "            bert_features = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Enhanced NaN/Inf checking for BERT features\n",
    "        if torch.isnan(bert_features).any() or torch.isinf(bert_features).any():\n",
    "            bert_features = torch.nan_to_num(bert_features, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        \n",
    "        return bert_features\n",
    "    \n",
    "    def process_tfidf(self, tfidf_features):\n",
    "        # Input sanitization\n",
    "        if torch.isnan(tfidf_features).any() or torch.isinf(tfidf_features).any():\n",
    "            tfidf_features = torch.nan_to_num(tfidf_features, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "\n",
    "        projected = self.tfidf_projection(tfidf_features)\n",
    "        \n",
    "        # Post-projection sanitization\n",
    "        if torch.isnan(projected).any() or torch.isinf(projected).any():\n",
    "            projected = torch.nan_to_num(projected, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        \n",
    "        if self.training and projected.size(0) > 1:\n",
    "            # Check for zero variance and add tiny noise if needed\n",
    "            var = projected.var(dim=0, keepdim=True)\n",
    "            zero_var_mask = var < 1e-8\n",
    "            if zero_var_mask.any():\n",
    "                noise = torch.randn_like(projected) * 1e-8\n",
    "                projected = projected + noise\n",
    "\n",
    "        normalized_tfidf = self.tfidf_norm(projected)\n",
    "        \n",
    "        if torch.isnan(normalized_tfidf).any() or torch.isinf(normalized_tfidf).any():\n",
    "            normalized_tfidf = torch.nan_to_num(normalized_tfidf, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        \n",
    "        return normalized_tfidf\n",
    "        \n",
    "    def forward(self, title, plot, tfidf_features=None, year=None, language_idx=None):      \n",
    "        features = []\n",
    "        text_batch = [f\"{t} [SEP] {p}\" for t, p in zip(title, plot)]\n",
    "        \n",
    "        if self.use_bert:\n",
    "            bert_features = self.process_bert(text_batch)  # Already handles NaNs\n",
    "            \n",
    "            # L2 normalization for BERT features\n",
    "            bert_norm_val = torch.norm(bert_features, p=2, dim=1, keepdim=True)\n",
    "            if torch.any(bert_norm_val < 1e-7):\n",
    "                noise = torch.randn_like(bert_features) * 1e-7\n",
    "                bert_features = torch.where((bert_norm_val < 1e-7).expand_as(bert_features),\n",
    "                                           bert_features + noise, bert_features)\n",
    "            bert_features = F.normalize(bert_features, p=2, dim=1, eps=1e-6)\n",
    "            features.append(bert_features)\n",
    "            \n",
    "        if self.use_tfidf and tfidf_features is not None:\n",
    "            tfidf_projected = self.process_tfidf(tfidf_features)\n",
    "            features.append(tfidf_projected)\n",
    "        elif self.use_tfidf and tfidf_features is None:\n",
    "            print(\"T- TF-IDF features were expected but not provided. Creating zeros tensor.\")\n",
    "            batch_size = len(title)\n",
    "            device = next(self.parameters()).device\n",
    "            tfidf_zeros = torch.zeros(batch_size, self.tfidf_dim, device=device)\n",
    "            features.append(tfidf_zeros)\n",
    "\n",
    "        if language_idx is not None:\n",
    "            if language_idx.dim() > 1:\n",
    "                language_idx = language_idx.squeeze(1)\n",
    "            lang_emb = self.language_embedding(language_idx)\n",
    "            features.append(lang_emb)\n",
    "\n",
    "        if year is not None:\n",
    "            features.append(year)\n",
    "\n",
    "        if not features:\n",
    "             print(\"T- WARNING: No text features were processed. Returning zeros.\")\n",
    "             batch_size = len(title) if title else 1\n",
    "             device = next(self.parameters()).device if len(list(self.parameters())) > 0 else 'cpu'\n",
    "             return torch.zeros(batch_size, self.output_dim, device=device)\n",
    "\n",
    "        combined = torch.cat(features, dim=1)\n",
    "        \n",
    "        if torch.isnan(combined).any() or torch.isinf(combined).any():\n",
    "            combined = torch.nan_to_num(combined, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        \n",
    "        output = self.fusion(combined)\n",
    "        \n",
    "        # Sanitize output from fusion MLP\n",
    "        if torch.isnan(output).any() or torch.isinf(output).any():\n",
    "            output = torch.nan_to_num(output, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        \n",
    "        # Check for zero vectors before normalization and handle them\n",
    "        output_norm = torch.norm(output, p=2, dim=1, keepdim=True)\n",
    "        \n",
    "        # Add noise to samples with near-zero norm\n",
    "        if torch.any(output_norm < 1e-7):\n",
    "            noise = torch.randn_like(output) * 1e-7\n",
    "            near_zero_mask = (output_norm < 1e-7).expand_as(output)\n",
    "            output = torch.where(near_zero_mask, output + noise, output)\n",
    "        \n",
    "        # Apply F.normalize with increased eps\n",
    "        output = F.normalize(output, p=2, dim=1, eps=1e-6)\n",
    "        \n",
    "        # Final check after F.normalize\n",
    "        if torch.isnan(output).any() or torch.isinf(output).any():\n",
    "            print(\"T- 👺 NaN/Inf in text_module output AFTER F.normalize. Re-clamping to zeros.\")\n",
    "            output_norm_check = torch.norm(output, p=2, dim=1, keepdim=True)\n",
    "            print(f\"   Input norms to F.normalize: min={output_norm_check.min().item():.8f}, \"\n",
    "                  f\"max={output_norm_check.max().item():.8f}, mean={output_norm_check.mean().item():.8f}\")\n",
    "            output = torch.zeros_like(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f73b20f-be1b-4ea7-bbaa-fcc783d2be98",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Fusion Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b781c5-170a-465e-a480-38c25ef4270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SIMPLIFIED!\n",
    "class SimpleFusionNetwork(nn.Module):\n",
    "    def __init__(self, visual_dim, audio_dim, text_dim, embedding_dim=128, ablation_mode='full'):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.ablation_mode = ablation_mode\n",
    "\n",
    "        self.fusion_input_dim = 0\n",
    "        if visual_dim > 0: self.fusion_input_dim += visual_dim\n",
    "        if (ablation_mode == 'visual_audio' or ablation_mode == 'full') and audio_dim > 0:\n",
    "            self.fusion_input_dim += audio_dim\n",
    "        if ablation_mode == 'full' and text_dim > 0:\n",
    "            self.fusion_input_dim += text_dim\n",
    "        \n",
    "        if self.fusion_input_dim == 0:\n",
    "            raise ValueError(\"Fusion input dimension cannot be zero.\")\n",
    "\n",
    "        print(f\"Fusion Input Dim: {self.fusion_input_dim}, Final Embedding Dim: {embedding_dim}\")\n",
    "\n",
    "        intermediate_dim = max(embedding_dim * 2, self.fusion_input_dim // 2)\n",
    "        intermediate_dim = min(intermediate_dim, 1024) # Cap the size\n",
    "\n",
    "        # A more stable MLP structure\n",
    "        self.fusion_mlp = nn.Sequential(\n",
    "            nn.LayerNorm(self.fusion_input_dim),\n",
    "            nn.Linear(self.fusion_input_dim, intermediate_dim),\n",
    "            nn.ReLU(),  # for stability\n",
    "            nn.Dropout(0.5), # Heavier dropout\n",
    "            nn.Linear(intermediate_dim, embedding_dim)\n",
    "        )\n",
    "        \n",
    "        self._initialize_weights()\n",
    "        print(f\"🔧 Robust SimpleFusionNetwork (LayerNorm -> Linear -> ReLU -> Dropout -> Linear) initialized.\")\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for module in self.fusion_mlp.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight, gain=0.7)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.LayerNorm):\n",
    "                nn.init.constant_(module.weight, 1)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward(self, visual_features, audio_features=None, text_features=None):\n",
    "        features_to_fuse = []\n",
    "        if visual_features is not None: features_to_fuse.append(visual_features)\n",
    "        \n",
    "        if (self.ablation_mode == 'visual_audio' or self.ablation_mode == 'full') and audio_features is not None:\n",
    "            features_to_fuse.append(audio_features)\n",
    "            \n",
    "        if self.ablation_mode == 'full' and text_features is not None:\n",
    "            features_to_fuse.append(text_features)\n",
    "        \n",
    "        if not features_to_fuse:\n",
    "            return torch.zeros((1, self.embedding_dim), device=visual_features.device if visual_features is not None else 'cpu')\n",
    "\n",
    "        fused_input = torch.cat(features_to_fuse, dim=1)\n",
    "        \n",
    "        # Check for NaN/Inf BEFORE the MLP\n",
    "        if torch.isnan(fused_input).any() or torch.isinf(fused_input).any():\n",
    "            print(\"F- ❌ NaN/Inf detected in fused_input before MLP. Clamping.\")\n",
    "            fused_input = torch.nan_to_num(fused_input, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "            \n",
    "        embedding = self.fusion_mlp(fused_input)\n",
    "        \n",
    "        # Final normalization ->l for contrastive loss\n",
    "        return F.normalize(embedding, p=2, dim=1, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886220d2-0e60-4ab0-a506-f9b23f77d54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REVISED HYBRID MODULE\n",
    "class HybridFusionNetwork(nn.Module):\n",
    "    def __init__(self, visual_dim, audio_dim, text_dim, embedding_dim=128, num_heads=4, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # --- Cross-Attention (Early Fusion) ---\n",
    "        # This part models interactions between modalities\n",
    "        self.visual_query = nn.Linear(visual_dim, visual_dim)\n",
    "        \n",
    "        # Audio influences Visual\n",
    "        self.audio_kv = nn.Linear(audio_dim, visual_dim * 2) # Key and Value from Audio\n",
    "        self.va_attention = nn.MultiheadAttention(embed_dim=visual_dim, num_heads=num_heads, dropout=dropout, batch_first=True)\n",
    "        self.va_norm = nn.LayerNorm(visual_dim)\n",
    "        \n",
    "        # Text influences Visual\n",
    "        self.text_kv = nn.Linear(text_dim, visual_dim * 2) # Key and Value from Text\n",
    "        self.vt_attention = nn.MultiheadAttention(embed_dim=visual_dim, num_heads=num_heads, dropout=dropout, batch_first=True)\n",
    "        self.vt_norm = nn.LayerNorm(visual_dim)\n",
    "        \n",
    "        # --- Late Fusion MLP ---\n",
    "        # This part combines the individually processed and the cross-attended features\n",
    "        \n",
    "        #will have the original visual, the visual-after-audio-attention, and visual-after-text-attention\n",
    "        late_fusion_input_dim = visual_dim * 3 \n",
    "        \n",
    "        self.late_fusion_mlp = nn.Sequential(\n",
    "            nn.LayerNorm(late_fusion_input_dim),\n",
    "            nn.Linear(late_fusion_input_dim, embedding_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(embedding_dim * 2, embedding_dim)\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "        print(f\"🔧 Revised HybridFusionNetwork initialized.\")\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight, gain=nn.init.calculate_gain('relu'))\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.LayerNorm):\n",
    "                nn.init.constant_(module.weight, 1)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward(self, visual_features, audio_features, text_features):\n",
    "        # All inputs should be pre-normalized from the sub-modules\n",
    "        \n",
    "        # --- Early Fusion Part (Cross-Attention) ---\n",
    "        # Visual features will act as the 'query' that asks questions of other modalities\n",
    "        q = self.visual_query(visual_features).unsqueeze(1) # (B, 1, D_vis)\n",
    "\n",
    "        # 1. Audio influences Visual\n",
    "        audio_k, audio_v = self.audio_kv(audio_features).chunk(2, dim=-1)\n",
    "        audio_k = audio_k.unsqueeze(1) # (B, 1, D_vis)\n",
    "        audio_v = audio_v.unsqueeze(1) # (B, 1, D_vis)\n",
    "        \n",
    "        # Attention: Visual asks \"what's in the audio?\"\n",
    "        va_out, _ = self.va_attention(query=q, key=audio_k, value=audio_v)\n",
    "        va_out = va_out.squeeze(1) # (B, D_vis)\n",
    "        visual_after_audio = self.va_norm(visual_features + va_out) \n",
    "\n",
    "        # 2. Text influences Visual\n",
    "        text_k, text_v = self.text_kv(text_features).chunk(2, dim=-1)\n",
    "        text_k = text_k.unsqueeze(1) # (B, 1, D_vis)\n",
    "        text_v = text_v.unsqueeze(1) # (B, 1, D_vis)\n",
    "\n",
    "        # Attention: Visual asks \"what's in the text?\"\n",
    "        vt_out, _ = self.vt_attention(query=q, key=text_k, value=text_v)\n",
    "        vt_out = vt_out.squeeze(1) # (B, D_vis)\n",
    "        visual_after_text = self.vt_norm(visual_features + vt_out) \n",
    "\n",
    "        # --- Late Fusion Part ---\n",
    "        # Combine the original visual with the attention-modified versions\n",
    "        late_fusion_input = torch.cat([visual_features, visual_after_audio, visual_after_text], dim=1)\n",
    "        \n",
    "        final_embedding = self.late_fusion_mlp(late_fusion_input)\n",
    "\n",
    "        # Final normalization before the loss function\n",
    "        return F.normalize(final_embedding, p=2, dim=1, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ed59d1-3476-4f5e-8dd4-62bdbc8c0e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UltraSimpleFusionNetwork(nn.Module):\n",
    "    def __init__(self, visual_dim, audio_dim, text_dim, embedding_dim=128, ablation_mode='full'):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.ablation_mode = ablation_mode\n",
    "\n",
    "        self.fusion_input_dim = 0\n",
    "        if visual_dim > 0: self.fusion_input_dim += visual_dim\n",
    "        if (ablation_mode == 'visual_audio' or ablation_mode == 'full') and audio_dim > 0:\n",
    "            self.fusion_input_dim += audio_dim\n",
    "        if ablation_mode == 'full' and text_dim > 0:\n",
    "            self.fusion_input_dim += text_dim\n",
    "        \n",
    "        if self.fusion_input_dim == 0:\n",
    "            raise ValueError(\"Fusion input dimension cannot be zero.\")\n",
    "\n",
    "        print(f\"Fusion Input Dim: {self.fusion_input_dim}, Final Embedding Dim: {embedding_dim}\")\n",
    "\n",
    "        # --- ULTRA-STABLE MLP ---\n",
    "        # A direct, normalized projection with no hidden layers.\n",
    "        # This is the most stable architecture possible.\n",
    "        self.fusion_mlp = nn.Sequential(\n",
    "            nn.LayerNorm(self.fusion_input_dim),\n",
    "            nn.Linear(self.fusion_input_dim, self.embedding_dim)\n",
    "        )\n",
    "        \n",
    "        self._initialize_weights()\n",
    "        print(f\"🔧 ULTRA-STABLE UltraSimpleFusionNetwork (LayerNorm -> Linear) initialized.\")\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for module in self.fusion_mlp.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                # Small gain for stability\n",
    "                nn.init.xavier_uniform_(module.weight, gain=0.1)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.LayerNorm):\n",
    "                nn.init.constant_(module.weight, 1)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward(self, visual_features, audio_features=None, text_features=None):\n",
    "        features_to_fuse = []\n",
    "        if visual_features is not None: features_to_fuse.append(visual_features)\n",
    "        \n",
    "        if (self.ablation_mode == 'visual_audio' or self.ablation_mode == 'full') and audio_features is not None:\n",
    "            features_to_fuse.append(audio_features)\n",
    "            \n",
    "        if self.ablation_mode == 'full' and text_features is not None:\n",
    "            features_to_fuse.append(text_features)\n",
    "        \n",
    "        if not features_to_fuse:\n",
    "            return torch.zeros((1, self.embedding_dim), device='cpu')\n",
    "\n",
    "        fused_input = torch.cat(features_to_fuse, dim=1)\n",
    "        \n",
    "        if torch.isnan(fused_input).any() or torch.isinf(fused_input).any():\n",
    "            fused_input = torch.nan_to_num(fused_input, nan=0.0)\n",
    "            \n",
    "        embedding = self.fusion_mlp(fused_input)\n",
    "        \n",
    "        return F.normalize(embedding, p=2, dim=1, eps=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a3806e-fcec-49d8-b549-8a83a4949124",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## LOSS Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9153f923-87a0-4a2b-b6cd-76e65e871e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLossCosine(nn.Module):\n",
    "    def __init__(self, margin=0.5): # A smaller margin is often used for Cosine distance\n",
    "        super(ContrastiveLossCosine, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.eps = 1e-9 # For numerical stability\n",
    "\n",
    "    def forward(self, embedding1, embedding2, label):\n",
    "        cosine_similarity = F.cosine_similarity(embedding1, embedding2, dim=1, eps=self.eps)\n",
    "\n",
    "        # Distance = 1 - Similarity\n",
    "        cosine_distance = 1 - cosine_similarity\n",
    "\n",
    "        loss_positive = (label) * torch.pow(cosine_distance, 2)\n",
    "        loss_negative = (1 - label) * torch.pow(torch.clamp(self.margin - cosine_distance, min=0.0), 2)\n",
    "        \n",
    "        loss_contrastive = torch.mean(loss_positive + loss_negative)\n",
    "\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9b258e-3ebb-46ae-945c-93b27626eb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLossEuclidean(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLossEuclidean, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.eps = 1e-9 # for stability in sqrt\n",
    "\n",
    "    def forward(self, embedding1, embedding2, label):\n",
    "        euclidean_distance = F.pairwise_distance(embedding1, embedding2, p=2, eps=self.eps)\n",
    "\n",
    "        loss_positive = (label) * torch.pow(euclidean_distance, 2)\n",
    "        loss_negative = (1 - label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
    "        loss_contrastive = torch.mean(loss_positive + loss_negative)\n",
    "        \n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0ac2ec-5dfb-44c0-92a0-6ae35c479e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLossCosine(nn.Module):\n",
    "    \"\"\"\n",
    "    Triplet loss function for cosine similarity.\n",
    "    The goal is to make the anchor more similar to the positive than to the negative,\n",
    "    by at least a certain margin.\n",
    "    \n",
    "    Objective: sim(a, p) > sim(a, n) + margin\n",
    "    Loss: max(0, sim(a, n) - sim(a, p) + margin)\n",
    "    \"\"\"\n",
    "    def __init__(self, margin=0.3):\n",
    "        super(TripletLossCosine, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.eps = 1e-9 # For numerical stability\n",
    "\n",
    "    def forward(self, anchor_emb, positive_emb, negative_emb):\n",
    "        # All embeddings should be L2-normalized from the model\n",
    "        sim_pos = F.cosine_similarity(anchor_emb, positive_emb, dim=1, eps=self.eps)\n",
    "        sim_neg = F.cosine_similarity(anchor_emb, negative_emb, dim=1, eps=self.eps)\n",
    "        \n",
    "        loss = torch.clamp(sim_neg - sim_pos + self.margin, min=0.0)\n",
    "        \n",
    "        return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc16ceb-0104-4190-bed1-a13ea90e3d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLossEuclidean(nn.Module):\n",
    "    \"\"\"\n",
    "    Triplet loss function for Euclidean distance.\n",
    "    The goal is to make the anchor's distance to the positive smaller than its\n",
    "    distance to the negative, by at least a certain margin.\n",
    "    \n",
    "    Objective: dist(a, p) + margin < dist(a, n)\n",
    "    Loss: max(0, dist(a, p) - dist(a, n) + margin)\n",
    "    \"\"\"\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(TripletLossEuclidean, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor_emb, positive_emb, negative_emb):\n",
    "        dist_pos = F.pairwise_distance(anchor_emb, positive_emb, p=2)\n",
    "        dist_neg = F.pairwise_distance(anchor_emb, negative_emb, p=2)\n",
    "        \n",
    "        loss = torch.clamp(dist_pos - dist_neg + self.margin, min=0.0)\n",
    "        \n",
    "        return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135d573b-94ee-403d-a2aa-da7f7dc91710",
   "metadata": {},
   "source": [
    "## SNN architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9766da-2080-41f3-9a27-3f94ff1c41cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedPairBasedSiameseNetwork(nn.Module):\n",
    "    def __init__(self, visual_module, audio_module, text_module, fusion_network):\n",
    "        super().__init__()\n",
    "        self.visual_module = visual_module\n",
    "        self.audio_module = audio_module\n",
    "        self.text_module = text_module\n",
    "        self.fusion_network = fusion_network\n",
    "    \n",
    "    def forward_single(self, \n",
    "                       frames_for_low_level,         # For visual low-level\n",
    "                       dino_frame_embeddings,        # For visual high-level (precomputed)\n",
    "                       vggish_embedding=None,        # For audio (precomputed VGGish)\n",
    "                       waveform_for_spec_cnn=None,   # For audio (raw wave for spec CNN)\n",
    "                       title=None, plot=None, tfidf_features=None, # For text\n",
    "                       year=None, language_idx=None):\n",
    "\n",
    "        # 1. Visual Processing\n",
    "        visual_features = self.visual_module(\n",
    "            frames=frames_for_low_level,\n",
    "            precomputed_dino_embeddings=dino_frame_embeddings\n",
    "        )\n",
    "        \n",
    "        # 2. Audio Processing\n",
    "        audio_features = None\n",
    "        if self.audio_module is not None:\n",
    "            audio_features = self.audio_module(\n",
    "                precomputed_vggish_embedding=vggish_embedding, \n",
    "                waveform_for_spec_cnn=waveform_for_spec_cnn\n",
    "            )\n",
    "        \n",
    "        # 3. Text Processing\n",
    "        text_features = None\n",
    "        if self.text_module is not None:\n",
    "            # check that title and plot are not None before passing them\n",
    "            if title is not None and plot is not None:\n",
    "                text_features = self.text_module(\n",
    "                    title=title, \n",
    "                    plot=plot, \n",
    "                    tfidf_features=tfidf_features, \n",
    "                    year=year, \n",
    "                    language_idx=language_idx\n",
    "                )\n",
    "            else:\n",
    "                batch_size = visual_features.shape[0]\n",
    "                device = visual_features.device\n",
    "                text_features = torch.zeros(batch_size, self.text_module.output_dim, device=device)\n",
    "\n",
    "            \n",
    "        fused_embedding = self.fusion_network(visual_features, audio_features, text_features)\n",
    "        return fused_embedding\n",
    "        \n",
    "    def forward(self, \n",
    "                frames1_for_low_level, dino_frame_embeddings1, waveform1, vggish_embedding1, \n",
    "                frames2_for_low_level, dino_frame_embeddings2, waveform2, vggish_embedding2,\n",
    "                video1_title=None, video1_plot=None, video1_tfidf=None, video1_year=None, video1_language_idx=None,\n",
    "                video2_title=None, video2_plot=None, video2_tfidf=None, video2_year=None, video2_language_idx=None):\n",
    "       \n",
    "        current_batch_size = frames1_for_low_level.shape[0]\n",
    "        current_device = frames1_for_low_level.device\n",
    "\n",
    "        nan_check_visual = (\n",
    "            torch.isnan(frames1_for_low_level).any() or torch.isnan(dino_frame_embeddings1).any() or\n",
    "            torch.isnan(frames2_for_low_level).any() or torch.isnan(dino_frame_embeddings2).any()\n",
    "        )\n",
    "        if nan_check_visual:\n",
    "            emb_dim = self.fusion_network.embedding_dim if hasattr(self.fusion_network, 'embedding_dim') else 128\n",
    "            return (torch.zeros(current_batch_size, emb_dim, device=current_device),\n",
    "                    torch.zeros(current_batch_size, emb_dim, device=current_device))\n",
    "\n",
    "        if self.audio_module:\n",
    "            waveform1 = torch.nan_to_num(waveform1, nan=0.0, posinf=0.0, neginf=0.0) if waveform1 is not None else None\n",
    "            vggish_embedding1 = torch.nan_to_num(vggish_embedding1, nan=0.0, posinf=0.0, neginf=0.0) if vggish_embedding1 is not None else None\n",
    "            waveform2 = torch.nan_to_num(waveform2, nan=0.0, posinf=0.0, neginf=0.0) if waveform2 is not None else None\n",
    "            vggish_embedding2 = torch.nan_to_num(vggish_embedding2, nan=0.0, posinf=0.0, neginf=0.0) if vggish_embedding2 is not None else None\n",
    "        if self.text_module:\n",
    "            video1_tfidf = torch.nan_to_num(video1_tfidf, nan=0.0) if video1_tfidf is not None else None\n",
    "            video2_tfidf = torch.nan_to_num(video2_tfidf, nan=0.0) if video2_tfidf is not None else None\n",
    "\n",
    "\n",
    "        embedding1 = self.forward_single( \n",
    "            frames1_for_low_level, dino_frame_embeddings1,\n",
    "            vggish_embedding1, waveform1, \n",
    "            video1_title, video1_plot, tfidf_features=video1_tfidf,\n",
    "            year=video1_year, language_idx=video1_language_idx\n",
    "        )\n",
    "        \n",
    "        embedding2 = self.forward_single(\n",
    "            frames2_for_low_level, dino_frame_embeddings2,\n",
    "            vggish_embedding2, waveform2, \n",
    "            video2_title, video2_plot, tfidf_features=video2_tfidf,\n",
    "            year=video2_year, language_idx=video2_language_idx \n",
    "        )\n",
    "\n",
    "        return embedding1, embedding2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f3b220-90de-452d-b951-1c77cec3bf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnlineTripletSiameseNetwork(nn.Module):\n",
    "    def __init__(self, visual_module, audio_module, text_module, fusion_network):\n",
    "        \"\"\"\n",
    "        Initializes the single processing tower of the Siamese network.\n",
    "        Its only job is to process ONE batch of movies into embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.visual_module = visual_module\n",
    "        self.audio_module = audio_module\n",
    "        self.text_module = text_module\n",
    "        self.fusion_network = fusion_network\n",
    "\n",
    "    def forward(self, **batch_data):\n",
    "        \"\"\"\n",
    "        Processes a BATCH of movie data into a BATCH of embeddings.\n",
    "\n",
    "        Args:\n",
    "            **batch_data: A dictionary of features. For a batch of size 16,\n",
    "                          'dino_frame_embeddings' would have a shape of [16, 16, 384],\n",
    "                          'vggish_embedding' would be [16, 128],\n",
    "                          'title' would be a list of 16 strings, etc.\n",
    "\n",
    "        Returns:\n",
    "            A tensor of embeddings with shape [batch_size, embedding_dim].\n",
    "        \"\"\"        \n",
    "        # 1. Process Visual Features\n",
    "        visual_features = self.visual_module(\n",
    "            frames=batch_data['frames_for_low_level'],\n",
    "            precomputed_dino_embeddings=batch_data['dino_frame_embeddings']\n",
    "        )\n",
    "        \n",
    "        # 2. Process Audio Features\n",
    "        audio_features = None\n",
    "        if self.audio_module is not None:\n",
    "            audio_features = self.audio_module(\n",
    "                precomputed_vggish_embedding=batch_data.get('vggish_embedding'),\n",
    "                waveform_for_spec_cnn=batch_data.get('waveform_for_spec_cnn')\n",
    "            )\n",
    "        \n",
    "        # 3. Process Text Features\n",
    "        text_features = None\n",
    "        if self.text_module is not None:\n",
    "            text_features = self.text_module(\n",
    "                title=batch_data.get('title'), \n",
    "                plot=batch_data.get('plot'),\n",
    "                tfidf_features=batch_data.get('tfidf_features'),\n",
    "                year=batch_data.get('year'),\n",
    "                language_idx=batch_data.get('language_idx')\n",
    "            )\n",
    "            \n",
    "        # 4. Fuse all features into the final embedding\n",
    "        raw_embedding = self.fusion_network(visual_features, audio_features, text_features)\n",
    "        \n",
    "        # Apply the final L2 normalization ->guarantees the input to loss function is always normalized.\n",
    "        final_normalized_embedding = F.normalize(raw_embedding, p=2, dim=1, eps=1e-6)\n",
    "        \n",
    "        return final_normalized_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad04fac2-cf7b-43e6-b50c-be32ec156825",
   "metadata": {},
   "source": [
    "## Dataset and Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c09e433-1a1a-41b7-86ef-8436f60cd42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDatasetS3:\n",
    "    \"\"\"Helper class to download videos from S3\"\"\"\n",
    "    def __init__(self, bucket_name):\n",
    "        self.s3_client = boto3.client('s3')\n",
    "        self.bucket_name = bucket_name\n",
    "        self.cache = CachedS3Dataset(bucket_name)\n",
    "        # Cache of movie IDs to full S3 keys\n",
    "        self.movie_key_cache = {}\n",
    "        \n",
    "    def get_video_path(self, movie_id, title=\"\"):\n",
    "        \"\"\"Download video from S3 and return local path\"\"\"\n",
    "        # Check if we've already found this movie's key\n",
    "        if movie_id in self.movie_key_cache:\n",
    "            video_key = self.movie_key_cache[movie_id]\n",
    "        else:\n",
    "            # If title is empty, we need to find the video by listing objects with the movie_id prefix\n",
    "            if not title:\n",
    "                try:\n",
    "                    # List objects with the movie ID prefix\n",
    "                    response = self.s3_client.list_objects_v2(\n",
    "                        Bucket=self.bucket_name,\n",
    "                        Prefix=f\"movie_trailers/{movie_id}_\"\n",
    "                    )\n",
    "                    \n",
    "                    # Check if any objects were found\n",
    "                    if 'Contents' in response and response['Contents']:\n",
    "                        # Use the first matching object\n",
    "                        video_key = response['Contents'][0]['Key']\n",
    "                        # Save to cache\n",
    "                        self.movie_key_cache[movie_id] = video_key\n",
    "                    else:\n",
    "                        print(f\"No video found for movie ID {movie_id}\")\n",
    "                        return None\n",
    "                except Exception as e:\n",
    "                    print(f\"Error listing objects for movie ID {movie_id}: {e}\")\n",
    "                    return None\n",
    "            else:\n",
    "                # If title is provided, use it to form the key\n",
    "                video_key = f\"movie_trailers/{movie_id}_{title}.mp4\"\n",
    "                self.movie_key_cache[movie_id] = video_key\n",
    "        \n",
    "        # Extract the filename from the key\n",
    "        filename = os.path.basename(video_key)\n",
    "        local_path = os.path.join(tempfile.gettempdir(), filename)\n",
    "        \n",
    "        # Check if file exists locally\n",
    "        if os.path.exists(local_path):\n",
    "            return local_path\n",
    "        \n",
    "        # Try to get from cache first, then download if not in cache\n",
    "        try:\n",
    "            # THIS IS THE KEY CHANGE: Use the cache to get the file content\n",
    "            file_content = self.cache.get(video_key)\n",
    "            \n",
    "            if file_content is not None:\n",
    "                # Write cache content to local file\n",
    "                with open(local_path, 'wb') as f:\n",
    "                    f.write(file_content)\n",
    "                return local_path\n",
    "            \n",
    "            # If not in cache, download directly\n",
    "            self.s3_client.download_file(self.bucket_name, video_key, local_path)\n",
    "            print(f\"Downloaded {video_key} to {local_path}\")\n",
    "            return local_path\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading video {video_key}: {e}\")\n",
    "            return None\n",
    "            \n",
    "    def cleanup_temp_files(self):\n",
    "        \"\"\"Remove all temporary downloaded video files to free space\"\"\"\n",
    "        temp_dir = tempfile.gettempdir()\n",
    "        for filename in os.listdir(temp_dir):\n",
    "            if filename.startswith(\"_\") and (filename.endswith(\".mp4\") or filename.endswith(\".avi\")):\n",
    "                try:\n",
    "                    os.remove(os.path.join(temp_dir, filename))\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not remove {filename}: {e}\")\n",
    "\n",
    "    #TODO test with different frame nbs (16-32-64(max))\n",
    "    def get_preextracted_frames(self, movie_id, num_frames=16):\n",
    "        \"\"\"Get pre-extracted frames from S3\"\"\"\n",
    "        try:\n",
    "            # NEW PATH: Updated to use the tensor files\n",
    "            frames_key = f\"movie_trailers_frames_tensors/{movie_id}_frames.pt\"\n",
    "            \n",
    "            with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
    "                self.s3_client.download_file(self.bucket_name, frames_key, temp_file.name)\n",
    "                frames = torch.load(temp_file.name, map_location='cpu')\n",
    "                os.unlink(temp_file.name)\n",
    "                \n",
    "            # have 64 frames but only need 16 -> sample evenly\n",
    "            if frames.shape[0] > num_frames:\n",
    "                indices = torch.linspace(0, frames.shape[0] - 1, num_frames).long()\n",
    "                frames = frames[indices]\n",
    "                \n",
    "            # # Optional: Print debug info for 1% of loads\n",
    "            # if torch.rand(1).item() < 0.01:  # 1% of loads\n",
    "            #     print(f\"📹 Loaded {frames.shape[0]} frames for movie {movie_id}\")\n",
    "                \n",
    "            return frames\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading pre-extracted frames for {movie_id}: {e}\")\n",
    "            # Fallback to zeros\n",
    "            return torch.zeros((num_frames, 3, 224, 224))\n",
    "\n",
    "    def get_preextracted_dino_frame_embeddings(self, movie_id):\n",
    "        try:\n",
    "            embedding_key = f\"{DINO_PER_FRAME_EMBEDDING_S3_PREFIX}{movie_id}_dino_per_frame.pt\"\n",
    "            \n",
    "            # Use the cache\n",
    "            file_content_bytes = self.cache.get(embedding_key) # self.cache is CachedS3Dataset instance\n",
    "            if file_content_bytes is None:\n",
    "                # This means it wasn't in cache and download failed, or key doesn't exist\n",
    "                print(f\"DINO per-frame emb for {movie_id} not found via cache or S3 for key {embedding_key}\")\n",
    "                raise FileNotFoundError(f\"DINO per-frame emb {embedding_key} not found.\")\n",
    "\n",
    "            dino_embeddings = torch.load(io.BytesIO(file_content_bytes), map_location='cpu')\n",
    "            # Ensure consistent number of frames if necessary, though DINO per-frame should match NUM_FRAMES_FROM_PT\n",
    "            if dino_embeddings.shape[0] != NUM_FRAMES_FROM_PT:\n",
    "                 print(f\"Warning: DINO per-frame for {movie_id} has {dino_embeddings.shape[0]} frames, expected {NUM_FRAMES_FROM_PT}. Adjusting/Padding.\")\n",
    "                 if dino_embeddings.shape[0] > NUM_FRAMES_FROM_PT:\n",
    "                     indices = torch.linspace(0, dino_embeddings.shape[0] - 1, NUM_FRAMES_FROM_PT).long()\n",
    "                     dino_embeddings = dino_embeddings[indices]\n",
    "                 else:\n",
    "                     padding_shape = (NUM_FRAMES_FROM_PT - dino_embeddings.shape[0], dino_embeddings.shape[1])\n",
    "                     padding = torch.zeros(padding_shape, dtype=dino_embeddings.dtype, device=dino_embeddings.device)\n",
    "                     dino_embeddings = torch.cat((dino_embeddings, padding), dim=0)\n",
    "            return dino_embeddings\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading pre-extracted DINO per-frame embeddings for {movie_id} (key: {embedding_key if 'embedding_key' in locals() else 'UNKNOWN'}): {e}\")\n",
    "            return torch.zeros(NUM_FRAMES_FROM_PT, DINO_DIM)\n",
    "\n",
    "    def get_preextracted_audio_vggish_wave(self, movie_id):\n",
    "        try:\n",
    "            audio_key = f\"{AUDIO_VGGISH_WAVE_S3_PREFIX}{movie_id}_audio_vggish_wave.pt\"\n",
    "\n",
    "            file_content_bytes = self.cache.get(audio_key)\n",
    "            if file_content_bytes is None:\n",
    "                print(f\"Audio data for {movie_id} not found via cache or S3 for key {audio_key}\")\n",
    "                raise FileNotFoundError(f\"Audio data {audio_key} not found.\")\n",
    "\n",
    "            audio_data = torch.load(io.BytesIO(file_content_bytes), map_location='cpu')\n",
    "            \n",
    "            waveform = audio_data['waveform']\n",
    "            if waveform.ndim == 1:\n",
    "                waveform = waveform.unsqueeze(0) \n",
    "            return audio_data['vggish_embedding'], waveform\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading pre-extracted VGGish & waveform for {movie_id} (key: {audio_key if 'audio_key' in locals() else 'UNKNOWN'}): {e}\")\n",
    "            return torch.zeros(VGGISH_DIM), torch.zeros(1, SAMPLE_RATE * MAX_AUDIO_LENGTH_SEC)\n",
    "\n",
    "        \n",
    "class TextDataset:\n",
    "    \"\"\"Helper class to manage text data\"\"\"\n",
    "    def __init__(self, csv_path, lang_vocab=None):\n",
    "        self.s3_client = boto3.client('s3')\n",
    "        \n",
    "        # Parse S3 path\n",
    "        bucket_name = csv_path.split('//')[1].split('/')[0]\n",
    "        key = '/'.join(csv_path.split('//')[1].split('/')[1:])\n",
    "        \n",
    "        # Download the CSV file\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix='.csv') as temp_file:\n",
    "            self.s3_client.download_file(bucket_name, key, temp_file.name)\n",
    "            self.text_df = pd.read_csv(temp_file.name)\n",
    "        \n",
    "        # Set index to movieId for faster lookups\n",
    "        self.text_df.set_index('movieId', inplace=True)\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        # Initialize TF-IDF vectorizer\n",
    "        self.tfidf = TfidfVectorizer(max_features=5000)\n",
    "        # Fit TF-IDF on plots\n",
    "        self.tfidf.fit(self.text_df['plot'].fillna('').values)\n",
    "    \n",
    "    # --- NEW: Create Language Vocabulary ---\n",
    "        # Find all unique languages by splitting the 'language' column\n",
    "        # --- MODIFIED: Use pre-built vocab or create a new one ---\n",
    "        if lang_vocab:\n",
    "            self.lang_vocab = lang_vocab\n",
    "        else:\n",
    "            # This will only be run once by the main process\n",
    "            print(\"Building new language vocabulary from scratch...\")\n",
    "            all_langs = set(['']) # <<< Add an empty string to guarantee index 0 exists\n",
    "            self.text_df['language'].dropna().str.split('|').apply(lambda langs: all_langs.update(set(langs)))\n",
    "            self.lang_vocab = {lang: i for i, lang in enumerate(sorted(list(all_langs)))}\n",
    "        \n",
    "        self.num_languages = len(self.lang_vocab)\n",
    "        # --- END MODIFICATION ---\n",
    "        print(f\"🌍 Created language vocabulary with {self.num_languages} languages.\")\n",
    "\n",
    "        # --- NEW: Prepare for Year Normalization ---\n",
    "        self.min_year = self.text_df['year'].min()\n",
    "        self.max_year = self.text_df['year'].max()\n",
    "        print(f\"🗓️ Year range for normalization: {self.min_year} - {self.max_year}\")\n",
    "\n",
    "    def get_text_features(self, movie_id):\n",
    "        \"\"\"Get text, year, and language features for a given movie ID.\"\"\"\n",
    "        try:\n",
    "            movie_data = self.text_df.loc[movie_id]\n",
    "            title = movie_data.get('title', \"\")\n",
    "            plot = movie_data.get('plot', \"\")\n",
    "            \n",
    "            # --- NEW: Process Year ---\n",
    "            year = movie_data.get('year', self.min_year) # Default to min_year if missing\n",
    "            # Normalize year to be roughly in [0, 1] range\n",
    "            normalized_year = (year - self.min_year) / (self.max_year - self.min_year + 1e-6)\n",
    "            year_tensor = torch.tensor([normalized_year], dtype=torch.float32)\n",
    "\n",
    "            # --- NEW: Process Language ---\n",
    "            # For simplicity, we use the first language if multiple are listed\n",
    "            # lang_str = movie_data.get('language', \"\").split('|')[0]\n",
    "            # lang_idx = self.lang_vocab.get(lang_str, 0) # Default to the first lang if not found\n",
    "            # lang_tensor = torch.tensor([lang_idx], dtype=torch.long)\n",
    "            # --- THIS IS THE FIX for the AttributeError ---\n",
    "            # Safely handle Language\n",
    "            # --- Make language processing more robust ---\n",
    "            lang_val = movie_data.get('language')\n",
    "            if isinstance(lang_val, str):\n",
    "                lang_str = lang_val.split('|')[0]\n",
    "            else:\n",
    "                lang_str = \"\" # Default to empty string if NaN or not a string\n",
    "    \n",
    "            lang_idx = self.lang_vocab.get(lang_str, self.lang_vocab.get(\"\", 0))\n",
    "            lang_tensor = torch.tensor([lang_idx], dtype=torch.long)\n",
    "\n",
    "            # BERT and TF-IDF processing (as before)\n",
    "            full_text = f\"{title} [SEP] {plot}\"\n",
    "            bert_tokens = self.tokenizer(full_text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "            tfidf_vector = self.tfidf.transform([plot]).toarray()[0]\n",
    "            tfidf_tensor = torch.tensor(tfidf_vector, dtype=torch.float32)\n",
    "            \n",
    "            return {\n",
    "                'title': title,\n",
    "                'plot': plot,\n",
    "                'tfidf_features': tfidf_tensor,\n",
    "                'year': year_tensor, # <<< NEW\n",
    "                'language_idx': lang_tensor # <<< NEW\n",
    "            }\n",
    "        except KeyError:\n",
    "            # Return default values if movie is not found\n",
    "            return {\n",
    "                'title': \"\", 'plot': \"\",\n",
    "                'tfidf_features': torch.zeros(5000, dtype=torch.float32),\n",
    "                'year': torch.tensor([0.5], dtype=torch.float32), # Default normalized year\n",
    "                'language_idx': torch.tensor([0], dtype=torch.long) # Default language index\n",
    "            }\n",
    "\n",
    "def extract_video_frames(video_path, num_frames=16):\n",
    "    \"\"\"Extract frames from a video file\"\"\"\n",
    "    frames = []\n",
    "    if video_path is None or not os.path.exists(video_path):\n",
    "        # Return zeros if video doesn't exist\n",
    "        return torch.zeros((num_frames, 3, 224, 224))\n",
    "    \n",
    "    try:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        if frame_count <= 0:\n",
    "            cap.release()\n",
    "            return torch.zeros((num_frames, 3, 224, 224))\n",
    "        \n",
    "        # Calculate frame indices to extract (evenly distributed)\n",
    "        indices = np.linspace(0, frame_count - 1, num_frames, dtype=int)\n",
    "        \n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        for idx in indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                # Convert BGR to RGB\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                # Apply transformations\n",
    "                frame = transform(frame)\n",
    "                frames.append(frame)\n",
    "            else:\n",
    "                # If frame reading fails, add zeros\n",
    "                frames.append(torch.zeros((3, 224, 224)))\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        # If we couldn't get enough frames, pad with zeros\n",
    "        while len(frames) < num_frames:\n",
    "            frames.append(torch.zeros((3, 224, 224)))\n",
    "            \n",
    "        return torch.stack(frames)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting frames from {video_path}: {e}\")\n",
    "        return torch.zeros((num_frames, 3, 224, 224))\n",
    "\n",
    "def extract_audio_features(video_path, max_length=10, sr=16000):\n",
    "    \"\"\"Extract audio features from a video file\"\"\"\n",
    "    if video_path is None or not os.path.exists(video_path):\n",
    "        # Return zeros if video doesn't exist\n",
    "        return {\n",
    "            'waveform': torch.zeros((1, sr * max_length)),\n",
    "            'spectrogram': torch.zeros((128, 100))\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        # Extract audio using librosa\n",
    "        y, _ = librosa.load(video_path, sr=sr, mono=True, duration=max_length)\n",
    "        \n",
    "        # Pad if audio is shorter than max_length\n",
    "        if len(y) < sr * max_length:\n",
    "            padding = sr * max_length - len(y)\n",
    "            y = np.pad(y, (0, padding), mode='constant')\n",
    "        # Trim if audio is longer than max_length\n",
    "        else:\n",
    "            y = y[:sr * max_length]\n",
    "        \n",
    "        # Create mel spectrogram\n",
    "        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        \n",
    "        # Normalize\n",
    "        log_mel_spec = (log_mel_spec - log_mel_spec.mean()) / (log_mel_spec.std() + 1e-8)\n",
    "        \n",
    "        # Resize spectrogram to fixed size (128, 100)\n",
    "        h, w = log_mel_spec.shape\n",
    "        if w > 100:\n",
    "            log_mel_spec = log_mel_spec[:, :100]\n",
    "        else:\n",
    "            padding = np.zeros((h, 100 - w))\n",
    "            log_mel_spec = np.concatenate([log_mel_spec, padding], axis=1)\n",
    "        \n",
    "        return {\n",
    "            'waveform': torch.tensor(y, dtype=torch.float32).unsqueeze(0),\n",
    "            'spectrogram': torch.tensor(log_mel_spec, dtype=torch.float32)\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting audio from {video_path}: {e}\")\n",
    "        # Add more detailed error info for debugging\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        return {\n",
    "            'waveform': torch.zeros((1, sr * max_length)),\n",
    "            'spectrogram': torch.zeros((128, 100))\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a8dc4e-c2a3-4f3b-b8fd-7f0b917d6dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairDataset(Dataset):\n",
    "    \"\"\"Dataset for pair-based Siamese network\"\"\"\n",
    "    def __init__(self, data_path, bucket_name, text_csv_path, ablation_mode, split='train', lang_vocab=None):\n",
    "        self.s3_client = boto3.client('s3')\n",
    "        self.bucket_name = bucket_name\n",
    "        self.split = split\n",
    "        self.ablation_mode = ablation_mode # Store it\n",
    "        \n",
    "        # Parse S3 path\n",
    "        s3_path_parts = data_path.split('/')\n",
    "        base_bucket = s3_path_parts[2]\n",
    "        base_prefix = '/'.join(s3_path_parts[3:])\n",
    "\n",
    "        base_prefix = '/'.join(s3_path_parts[3:])\n",
    "        base_prefix = base_prefix.rstrip('/')\n",
    "        \n",
    "        pos_key = f\"{base_prefix}/{split}_positive_pairs.pkl\"\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
    "            try:\n",
    "                self.s3_client.download_file(base_bucket, pos_key, temp_file.name)\n",
    "                with open(temp_file.name, 'rb') as f: \n",
    "                    self.positive_pairs = pickle.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading/loading positive pairs: {e}\")\n",
    "                print(f\"Attempted to access: s3://{base_bucket}/{pos_key}\")\n",
    "                raise\n",
    "        \n",
    "        # Download negative pairs\n",
    "        neg_key = f\"{base_prefix}/{split}_negative_pairs.pkl\"  \n",
    "        with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
    "            try:\n",
    "                self.s3_client.download_file(base_bucket, neg_key, temp_file.name)\n",
    "                with open(temp_file.name, 'rb') as f:\n",
    "                    self.negative_pairs = pickle.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading/loading negative pairs: {e}\")\n",
    "                print(f\"Attempted to access: s3://{base_bucket}/{neg_key}\")\n",
    "                raise\n",
    "        \n",
    "        # Combine positive and negative pairs\n",
    "        self.all_pairs = []\n",
    "        for pair in self.positive_pairs:\n",
    "            movie1_id, movie2_id = pair\n",
    "            self.all_pairs.append((movie1_id, movie2_id, 1))\n",
    "        for pair in self.negative_pairs:\n",
    "            movie1_id, movie2_id = pair\n",
    "            self.all_pairs.append((movie1_id, movie2_id, 0))\n",
    "            \n",
    "        # Initialize video and text helpers\n",
    "        self.video_dataset = VideoDatasetS3(bucket_name)\n",
    "        if self.ablation_mode == 'full':\n",
    "            self.text_dataset = TextDataset(text_csv_path, lang_vocab=lang_vocab)\n",
    "        else:\n",
    "            self.text_dataset = None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.all_pairs)\n",
    "\n",
    "    @lru_cache(maxsize=128) #try32 if not working\n",
    "    def _cached_get_video_frames(self, video_path):\n",
    "        return extract_video_frames(video_path)\n",
    "    \n",
    "    @lru_cache(maxsize=128)\n",
    "    def _cached_get_audio_features(self, video_path):\n",
    "        return extract_audio_features(video_path)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        movie1_id, movie2_id, label = self.all_pairs[idx]\n",
    "        \n",
    "        # --- Visual ---\n",
    "        # Load the original (normalized) frames for low-level feature calculation\n",
    "        frames1_for_low_level = self.video_dataset.get_preextracted_frames(movie1_id, num_frames=NUM_FRAMES_FROM_PT)\n",
    "        frames2_for_low_level = self.video_dataset.get_preextracted_frames(movie2_id, num_frames=NUM_FRAMES_FROM_PT)\n",
    "        \n",
    "        # Load pre-extracted DINOv2 per-frame embeddings\n",
    "        dino_frame_embeddings1 = self.video_dataset.get_preextracted_dino_frame_embeddings(movie1_id)\n",
    "        dino_frame_embeddings2 = self.video_dataset.get_preextracted_dino_frame_embeddings(movie2_id)\n",
    "\n",
    "        if torch.isnan(dino_frame_embeddings1).any():\n",
    "            print(f\"!!! NaN found in DINO embedding for movie_id: {movie1_id}\")\n",
    "    \n",
    "        # --- Audio ---\n",
    "        vggish_emb1, waveform1 = torch.zeros(VGGISH_DIM), torch.zeros(1, SAMPLE_RATE * MAX_AUDIO_LENGTH_SEC)\n",
    "        vggish_emb2, waveform2 = torch.zeros(VGGISH_DIM), torch.zeros(1, SAMPLE_RATE * MAX_AUDIO_LENGTH_SEC)\n",
    "    \n",
    "        if self.ablation_mode != 'visual_only':\n",
    "            vggish_emb1, waveform1 = self.video_dataset.get_preextracted_audio_vggish_wave(movie1_id)\n",
    "            vggish_emb2, waveform2 = self.video_dataset.get_preextracted_audio_vggish_wave(movie2_id)\n",
    "        \n",
    "        # --- Text ---\n",
    "        item = {\n",
    "            'movie1_id': movie1_id,\n",
    "            'movie2_id': movie2_id,\n",
    "            'frames1_for_low_level': frames1_for_low_level, \n",
    "            'dino_frame_embeddings1': dino_frame_embeddings1,\n",
    "            'frames2_for_low_level': frames2_for_low_level,\n",
    "            'dino_frame_embeddings2': dino_frame_embeddings2,\n",
    "            'vggish_embedding1': vggish_emb1,                \n",
    "            'waveform1': waveform1,                          \n",
    "            'vggish_embedding2': vggish_emb2,\n",
    "            'waveform2': waveform2,\n",
    "            'label': torch.tensor(label, dtype=torch.float32)\n",
    "        }\n",
    "        \n",
    "        if self.ablation_mode == 'full' and self.text_dataset is not None:\n",
    "            text1_data = self.text_dataset.get_text_features(movie1_id)\n",
    "            item['video1_title'] = text1_data['title']\n",
    "            item['video1_plot'] = text1_data['plot']\n",
    "            item['video1_tfidf'] = text1_data['tfidf_features']\n",
    "            item['video1_year'] = text1_data['year']            \n",
    "            item['video1_language_idx'] = text1_data['language_idx'] \n",
    "            \n",
    "            text2_data = self.text_dataset.get_text_features(movie2_id)\n",
    "            item['video2_title'] = text2_data['title']\n",
    "            item['video2_plot'] = text2_data['plot']\n",
    "            item['video2_tfidf'] = text2_data['tfidf_features']\n",
    "            item['video2_year'] = text2_data['year']        \n",
    "            item['video2_language_idx'] = text2_data['language_idx']\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48653808-4862-4948-bde4-11639bb6e843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# CLASS 1: FOR ONLINE TRAINING (provides Anchor-Positive pairs)\n",
    "# =================================================================\n",
    "class TripletDatasetForOnlineTraining(Dataset):\n",
    "    \"\"\"\n",
    "    This dataset is used ONLY for TRAINING.\n",
    "    It loads POSITIVE pairs and returns a dictionary for the anchor and a dictionary for the positive.\n",
    "    The negative is found \"online\" during the training loop.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path, bucket_name, text_csv_path, ablation_mode, split='train', lang_vocab=None):\n",
    "        self.s3_client = boto3.client('s3')\n",
    "        self.bucket_name = bucket_name\n",
    "        self.split = split\n",
    "        self.ablation_mode = ablation_mode\n",
    "\n",
    "        s3_path_parts = data_path.split('/')\n",
    "        if len(s3_path_parts) < 3:\n",
    "             raise ValueError(f\"Invalid S3 path format: {data_path}\")\n",
    "        \n",
    "        base_bucket = s3_path_parts[2]\n",
    "        base_prefix = '/'.join(s3_path_parts[3:]).rstrip('/')\n",
    "        \n",
    "        # This dataset loads POSITIVE pairs to serve as (Anchor, Positive)\n",
    "        pos_key = f\"{base_prefix}/{split}_positive_pairs.pkl\"\n",
    "        \n",
    "        print(f\"Loading (Anchor, Positive) pairs from: s3://{base_bucket}/{pos_key}\")\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".pkl\") as temp_file:\n",
    "            try:\n",
    "                self.s3_client.download_file(base_bucket, pos_key, temp_file.name)\n",
    "                with open(temp_file.name, 'rb') as f:\n",
    "                    self.anchor_positive_pairs = pickle.load(f)\n",
    "                print(f\"Successfully loaded {len(self.anchor_positive_pairs)} anchor-positive pairs for '{split}' split.\")\n",
    "            except Exception as e:\n",
    "                print(f\"FATAL ERROR: Could not download/load positive pairs for online training from s3://{base_bucket}/{pos_key}\")\n",
    "                print(f\"Error details: {e}\")\n",
    "                raise\n",
    "            finally:\n",
    "                os.unlink(temp_file.name)\n",
    "        \n",
    "        # Initialize helper classes\n",
    "        self.video_dataset = VideoDatasetS3(bucket_name)\n",
    "        if self.ablation_mode == 'full':\n",
    "            self.text_dataset = TextDataset(text_csv_path, lang_vocab=lang_vocab)\n",
    "        else:\n",
    "            self.text_dataset = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.anchor_positive_pairs)\n",
    "    \n",
    "    def _get_single_item_features(self, movie_id):\n",
    "        \"\"\"Helper function to load all features for a single movie_id.\"\"\"\n",
    "        \n",
    "        # --- Visual Features ---\n",
    "        frames_for_low_level = self.video_dataset.get_preextracted_frames(movie_id, num_frames=NUM_FRAMES_FROM_PT)\n",
    "        dino_frame_embeddings = self.video_dataset.get_preextracted_dino_frame_embeddings(movie_id)\n",
    "\n",
    "        # --- Audio Features ---\n",
    "        vggish_emb, waveform = torch.zeros(VGGISH_DIM), torch.zeros(1, SAMPLE_RATE * MAX_AUDIO_LENGTH_SEC)\n",
    "        if self.ablation_mode != 'visual_only':\n",
    "            vggish_emb, waveform = self.video_dataset.get_preextracted_audio_vggish_wave(movie_id)\n",
    "        \n",
    "        # --- Prepare the data dictionary ---\n",
    "        item_data = {\n",
    "            'movie_id': torch.tensor(movie_id, dtype=torch.long), # CRITICAL for online mining\n",
    "            'frames_for_low_level': frames_for_low_level,\n",
    "            'dino_frame_embeddings': dino_frame_embeddings,\n",
    "            'vggish_embedding': vggish_emb,\n",
    "            'waveform_for_spec_cnn': waveform\n",
    "        }\n",
    "\n",
    "        # --- Text Features (if applicable) ---\n",
    "        if self.ablation_mode == 'full' and self.text_dataset is not None:\n",
    "            text_data = self.text_dataset.get_text_features(movie_id)\n",
    "            item_data.update(text_data)\n",
    "        else:\n",
    "            # Provide None placeholders for model's forward signature if text isn't used\n",
    "            item_data.update({'title': None, 'plot': None, 'tfidf_features': None, 'year': None, 'language_idx': None})\n",
    "            \n",
    "        return item_data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # This returns the data for the anchor and the positive\n",
    "        anchor_id, positive_id = self.anchor_positive_pairs[idx]\n",
    "        anchor_data = self._get_single_item_features(anchor_id)\n",
    "        positive_data = self._get_single_item_features(positive_id)\n",
    "        \n",
    "        return {\"anchor_data\": anchor_data, \"positive_data\": positive_data}\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# CLASS 2: FOR VALIDATION (provides pre-generated A, P, N triplets)\n",
    "# ======================================================================\n",
    "class TripletDatasetForValidation(Dataset):\n",
    "    \"\"\"\n",
    "    This dataset is used ONLY for VALIDATION.\n",
    "    It loads the pre-generated, \"easy\" triplets file to provide a consistent\n",
    "    benchmark for measuring validation loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path, bucket_name, text_csv_path, ablation_mode, split='validation', lang_vocab=None):\n",
    "        self.s3_client = boto3.client('s3')\n",
    "        self.bucket_name = bucket_name\n",
    "        self.split = split\n",
    "        self.ablation_mode = ablation_mode\n",
    "\n",
    "        s3_path_parts = data_path.split('/')\n",
    "        if len(s3_path_parts) < 3:\n",
    "             raise ValueError(f\"Invalid S3 path format: {data_path}\")\n",
    "\n",
    "        base_bucket = s3_path_parts[2]\n",
    "        base_prefix = '/'.join(s3_path_parts[3:]).rstrip('/')\n",
    "        \n",
    "        # This dataset loads the pre-generated TRIPLETS\n",
    "        triplet_key = f\"{base_prefix}/{split}_triplets.pkl\"\n",
    "        \n",
    "        print(f\"Loading (A, P, N) triplets from: s3://{base_bucket}/{triplet_key}\")\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".pkl\") as temp_file:\n",
    "            try:\n",
    "                self.s3_client.download_file(base_bucket, triplet_key, temp_file.name)\n",
    "                with open(temp_file.name, 'rb') as f:\n",
    "                    self.triplets = pickle.load(f)\n",
    "                print(f\"Successfully loaded {len(self.triplets)} triplets for '{split}' split.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not load '{triplet_key}'. Trying 'all_triplets.pkl' instead...\")\n",
    "                triplet_key = f\"{base_prefix}/all_triplets.pkl\"\n",
    "                try:\n",
    "                    self.s3_client.download_file(base_bucket, triplet_key, temp_file.name)\n",
    "                    with open(temp_file.name, 'rb') as f:\n",
    "                        all_triplets = pickle.load(f)\n",
    "                    \n",
    "                    random.seed(42) # for reproducible splits\n",
    "                    random.shuffle(all_triplets)\n",
    "                    train_end = int(0.7 * len(all_triplets))\n",
    "                    val_end = int(0.85 * len(all_triplets))\n",
    "                    \n",
    "                    if split == 'train': self.triplets = all_triplets[:train_end]\n",
    "                    elif split == 'validation': self.triplets = all_triplets[train_end:val_end]\n",
    "                    else: self.triplets = all_triplets[val_end:]\n",
    "                    \n",
    "                    print(f\"Loaded from 'all_triplets.pkl' and took {len(self.triplets)} for '{split}' split.\")\n",
    "                    \n",
    "                except Exception as e2:\n",
    "                    print(f\"FATAL ERROR: Could not download/load triplets from either key. Error: {e2}\")\n",
    "                    raise\n",
    "            finally:\n",
    "                os.unlink(temp_file.name)\n",
    "\n",
    "        # Initialize helper classes (same as the training dataset)\n",
    "        self.video_dataset = VideoDatasetS3(bucket_name)\n",
    "        if self.ablation_mode == 'full':\n",
    "            self.text_dataset = TextDataset(text_csv_path, lang_vocab=lang_vocab)\n",
    "        else:\n",
    "            self.text_dataset = None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "\n",
    "    def _get_single_item_features(self, movie_id):\n",
    "        \"\"\"Helper function to load all features for a single movie_id.\"\"\"\n",
    "        item_data = {\n",
    "            'movie_id': torch.tensor(movie_id, dtype=torch.long),\n",
    "            'frames_for_low_level': self.video_dataset.get_preextracted_frames(movie_id, num_frames=NUM_FRAMES_FROM_PT),\n",
    "            'dino_frame_embeddings': self.video_dataset.get_preextracted_dino_frame_embeddings(movie_id),\n",
    "            'vggish_embedding': torch.zeros(VGGISH_DIM),\n",
    "            'waveform_for_spec_cnn': torch.zeros(1, SAMPLE_RATE * MAX_AUDIO_LENGTH_SEC)\n",
    "        }\n",
    "        if self.ablation_mode != 'visual_only':\n",
    "            item_data['vggish_embedding'], item_data['waveform_for_spec_cnn'] = self.video_dataset.get_preextracted_audio_vggish_wave(movie_id)\n",
    "        \n",
    "        if self.ablation_mode == 'full' and self.text_dataset is not None:\n",
    "            text_data = self.text_dataset.get_text_features(movie_id)\n",
    "            item_data.update(text_data)\n",
    "        else:\n",
    "            item_data.update({'title': None, 'plot': None, 'tfidf_features': None, 'year': None, 'language_idx': None})\n",
    "            \n",
    "        return item_data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # This returns the data for the pre-generated anchor, positive, and negative\n",
    "        anchor_id, positive_id, negative_id = self.triplets[idx]\n",
    "        anchor_data = self._get_single_item_features(anchor_id)\n",
    "        positive_data = self._get_single_item_features(positive_id)\n",
    "        negative_data = self._get_single_item_features(negative_id)\n",
    "        \n",
    "        return {\n",
    "            \"anchor_data\": anchor_data,\n",
    "            \"positive_data\": positive_data,\n",
    "            \"negative_data\": negative_data\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8ffc78-93a2-4317-b7c7-3e18614e4d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllMoviesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset that returns individual movie items and their movie_id.\n",
    "    This is used for proper in-batch negative mining.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path, bucket_name, text_csv_path, ablation_mode, split='train', lang_vocab=None):\n",
    "        self.ablation_mode = ablation_mode\n",
    "        \n",
    "        print(f\"Initializing AllMoviesDataset for '{split}' split...\")\n",
    "        s3_client = boto3.client('s3')\n",
    "        s3_path_parts = data_path.split('/')\n",
    "        base_bucket = s3_path_parts[2]\n",
    "        base_prefix = '/'.join(s3_path_parts[3:]).rstrip('/')\n",
    "\n",
    "        all_movie_ids = set()\n",
    "        for pair_file in [f\"{split}_positive_pairs.pkl\", f\"{split}_negative_pairs.pkl\"]:\n",
    "            key = f\"{base_prefix}/{pair_file}\"\n",
    "            try:\n",
    "                with tempfile.NamedTemporaryFile(delete=False) as temp_f:\n",
    "                    s3_client.download_file(base_bucket, key, temp_f.name)\n",
    "                    with open(temp_f.name, 'rb') as f:\n",
    "                        pairs = pickle.load(f)\n",
    "                    for id1, id2 in pairs:\n",
    "                        all_movie_ids.add(id1)\n",
    "                        all_movie_ids.add(id2)\n",
    "                os.unlink(temp_f.name)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load {key}. It might not exist for this split. Error: {e}\")\n",
    "\n",
    "        self.movie_ids = sorted(list(all_movie_ids))\n",
    "        print(f\"Found {len(self.movie_ids)} unique movies for the '{split}' split.\")\n",
    "\n",
    "        self.video_dataset = VideoDatasetS3(bucket_name)\n",
    "        if self.ablation_mode == 'full':\n",
    "            self.text_dataset = TextDataset(text_csv_path, lang_vocab=lang_vocab)\n",
    "        else:\n",
    "            self.text_dataset = None\n",
    "\n",
    "        self._get_single_item_features = TripletDatasetForValidation._get_single_item_features.__get__(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.movie_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        movie_id = self.movie_ids[idx]\n",
    "        item_data = self._get_single_item_features(movie_id)\n",
    "        return item_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9c6d70-5d3b-47d6-a29e-2977e742c155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best=False, checkpoint_dir='checkpoints', s3_bucket='md-data-content-recommendation', s3_prefix='n2n-model/cosine-with-languages-checkpoints'):\n",
    "    \"\"\"\n",
    "    Save a model checkpoint both locally and to S3.\n",
    "    \n",
    "    Args:\n",
    "        state: The model state to save\n",
    "        is_best: Whether this is the best model so far\n",
    "        checkpoint_dir: Local directory to save checkpoints (temporary)\n",
    "        s3_bucket: S3 bucket name\n",
    "        s3_prefix: Prefix path in the S3 bucket\n",
    "    \"\"\"\n",
    "    # Create local directory if it doesn't exist\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Local filename\n",
    "    local_filename = os.path.join(checkpoint_dir, f\"epoch_{state['epoch']}.pth\")\n",
    "    \n",
    "    try:\n",
    "        # First try saving directly to S3 (preferred method)\n",
    "        s3_client = boto3.client('s3')\n",
    "        \n",
    "        # Save to a buffer first\n",
    "        buffer = io.BytesIO()\n",
    "        torch.save(state, buffer)\n",
    "        buffer.seek(0)\n",
    "        \n",
    "        # Upload to S3\n",
    "        s3_path = f\"{s3_prefix}/epoch_{state['epoch']}.pth\"\n",
    "        s3_client.upload_fileobj(buffer, s3_bucket, s3_path)\n",
    "        print(f\"Checkpoint saved to s3://{s3_bucket}/{s3_path}\")\n",
    "        \n",
    "        # If best model, save an additional copy\n",
    "        if is_best:\n",
    "            buffer.seek(0)  # Reset buffer position\n",
    "            best_s3_path = f\"{s3_prefix}/best_model.pth\"\n",
    "            s3_client.upload_fileobj(buffer, s3_bucket, best_s3_path)\n",
    "            print(f\"New best model saved to s3://{s3_bucket}/{best_s3_path}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to S3: {e}. Falling back to local save.\")\n",
    "        # Fall back to local save\n",
    "        torch.save(state, local_filename)\n",
    "        \n",
    "        if is_best:\n",
    "            best_path = os.path.join(checkpoint_dir, \"best_model.pth\")\n",
    "            torch.save(state, best_path)\n",
    "            print(f\"New best model saved to {best_path}\")\n",
    "\n",
    "def load_checkpoint_from_s3(s3_bucket, s3_key, map_location=None):\n",
    "    \"\"\"\n",
    "    Load a checkpoint directly from S3.\n",
    "    \n",
    "    Args:\n",
    "        s3_bucket: S3 bucket name\n",
    "        s3_key: S3 key (path to the checkpoint)\n",
    "        map_location: Optional device mapping for torch.load\n",
    "        \n",
    "    Returns:\n",
    "        The loaded checkpoint\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    buffer = io.BytesIO()\n",
    "    \n",
    "    print(f\"Loading checkpoint from s3://{s3_bucket}/{s3_key}\")\n",
    "    s3_client.download_fileobj(s3_bucket, s3_key, buffer)\n",
    "    buffer.seek(0)\n",
    "    \n",
    "    return torch.load(buffer, map_location=map_location, weights_only=False)\n",
    "\n",
    "def clean_local_checkpoints(checkpoint_dir='checkpoints', keep_latest=2):\n",
    "    \"\"\"\n",
    "    Remove old local checkpoints, keeping only the specified number of latest files.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_dir: Directory containing checkpoints\n",
    "        keep_latest: Number of latest checkpoints to keep\n",
    "    \"\"\"\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        return\n",
    "        \n",
    "    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith('epoch_') and f.endswith('.pth')]\n",
    "    if len(checkpoints) <= keep_latest:\n",
    "        return\n",
    "        \n",
    "    # Sort by epoch number\n",
    "    checkpoints.sort(key=lambda x: int(x.split('_')[1].split('.')[0]), reverse=True)\n",
    "    \n",
    "    # Delete older checkpoints\n",
    "    for checkpoint in checkpoints[keep_latest:]:\n",
    "        os.remove(os.path.join(checkpoint_dir, checkpoint))\n",
    "        print(f\"Removed old checkpoint: {checkpoint}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38408801-06f4-4eef-9a4f-d78e9588b4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CachedS3Dataset:\n",
    "    def __init__(self, bucket_name, cache_size=1000):\n",
    "        self.cache = OrderedDict()  # Use OrderedDict for LRU functionality\n",
    "        self.cache_size = cache_size\n",
    "        self.s3 = boto3.client('s3')\n",
    "        self.bucket_name = bucket_name\n",
    "        \n",
    "    def get(self, key):\n",
    "        if key in self.cache:\n",
    "            # Move this key to the end (most recently used position)\n",
    "            value = self.cache.pop(key)\n",
    "            self.cache[key] = value\n",
    "            return value\n",
    "        \n",
    "        # Download to a BytesIO object if not in cache\n",
    "        buffer = io.BytesIO()\n",
    "        try:\n",
    "            self.s3.download_fileobj(self.bucket_name, key, buffer)\n",
    "            buffer.seek(0)  # Reset pointer to start of buffer\n",
    "            data = buffer.read()  # Store the file content\n",
    "            \n",
    "            # If we've reached cache limit, remove least recently used item\n",
    "            if len(self.cache) >= self.cache_size:\n",
    "                self.cache.popitem(last=False)  # Remove first item (least recently used)\n",
    "                \n",
    "            self.cache[key] = data\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {key} to cache: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1bc21d-54ec-4d38-a0a9-2ec78cb6e04f",
   "metadata": {},
   "source": [
    "## Model Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acec8926-dbcf-4212-95eb-514a7d702a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_and_criterion(config, device, num_languages):\n",
    "    \"\"\"Builds the model, criterion, and optimizer from a config dict.\"\"\"\n",
    "    \n",
    "    # 1. Build the sub-modules (these are the same for all models)\n",
    "    visual_module = VisualProcessingModule(use_precomputed_embeddings=True, dinov2_embedding_dim=DINO_DIM)\n",
    "    audio_module = OptimizedAudioProcessingModule(use_vggish=True, vggish_embedding_dim=VGGISH_DIM)\n",
    "    text_module = TextProcessingModule(num_languages=num_languages)\n",
    "\n",
    "    # 2. Build the correct Fusion Network\n",
    "    fusion_config = config['fusion_network']\n",
    "    if fusion_config == 'Hybrid':\n",
    "        fusion_net = HybridFusionNetwork(visual_module.output_dim, audio_module.output_dim, text_module.output_dim)\n",
    "    elif fusion_config == 'Simple':\n",
    "        fusion_net = SimpleFusionNetwork(visual_module.output_dim, audio_module.output_dim, text_module.output_dim)\n",
    "    elif fusion_config == 'UltraSimple':\n",
    "        fusion_net = UltraSimpleFusionNetwork(visual_module.output_dim, audio_module.output_dim, text_module.output_dim)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown fusion network: {fusion_config}\")\n",
    "\n",
    "    # 3. Build the correct SNN architecture and Loss Function\n",
    "    model_type = config['model_type']\n",
    "    loss_type = config['loss_type']\n",
    "    \n",
    "    if model_type == 'pair':\n",
    "        model = OptimizedPairBasedSiameseNetwork(visual_module, audio_module, text_module, fusion_net)\n",
    "        if loss_type == 'cosine':\n",
    "            criterion = ContrastiveLossCosine(margin=0.8).to(device)\n",
    "        elif loss_type == 'euclidean':\n",
    "            criterion = ContrastiveLossEuclidean(margin=1.2).to(device)\n",
    "        else: raise ValueError(\"Invalid loss type for pair model\")\n",
    "            \n",
    "    elif model_type == 'triplet':\n",
    "        model = OnlineTripletSiameseNetwork(visual_module, audio_module, text_module, fusion_net)\n",
    "        if loss_type == 'cosine':\n",
    "            criterion = TripletLossCosine(margin=0.3).to(device)\n",
    "        elif loss_type == 'euclidean':\n",
    "            criterion = TripletLossEuclidean(margin=1.0).to(device)\n",
    "        else: raise ValueError(\"Invalid loss type for triplet model\")\n",
    "    \n",
    "    else: raise ValueError(f\"Unknown model_type: {model_type}\")\n",
    "\n",
    "    # 4. Create Optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['hyperparameters']['lr'], weight_decay=1e-2)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.7, verbose=True)\n",
    "\n",
    "    return model.to(device), criterion, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821f0782-8eef-4426-b767-4773b342c1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_small_subset_for_testing(dataset, size=1000):\n",
    "    \"\"\"\n",
    "    Create a small subset of the dataset for testing.\n",
    "    This function now handles both PairDataset and TripletDataset.\n",
    "    \"\"\"\n",
    "    random.seed(42)  # For reproducibility\n",
    "\n",
    "    # --- Check if the dataset is for pairs or triplets ---\n",
    "    if hasattr(dataset, 'all_pairs'):\n",
    "        # This is a PairDataset\n",
    "        print(f\"Subsetting a PairDataset...\")\n",
    "        \n",
    "        # Check current distribution\n",
    "        original_size = len(dataset.all_pairs)\n",
    "        current_positive = sum(1 for pair in dataset.all_pairs if pair[2] == 1)\n",
    "        current_negative = original_size - current_positive\n",
    "        print(f\"Original dataset - Size: {original_size}, Positive: {current_positive}, Negative: {current_negative}\")\n",
    "\n",
    "        # Separate positive and negative pairs\n",
    "        positive_pairs = [pair for pair in dataset.all_pairs if pair[2] == 1]\n",
    "        negative_pairs = [pair for pair in dataset.all_pairs if pair[2] == 0]\n",
    "\n",
    "        # Take half positive, half negative (or whatever is available)\n",
    "        half_size = size // 2\n",
    "        sampled_positive = random.sample(positive_pairs, min(half_size, len(positive_pairs)))\n",
    "        sampled_negative = random.sample(negative_pairs, min(half_size, len(negative_pairs)))\n",
    "        \n",
    "        # Combine and shuffle\n",
    "        dataset.all_pairs = sampled_positive + sampled_negative\n",
    "        random.shuffle(dataset.all_pairs)\n",
    "        print(f\"New subset size: {len(dataset.all_pairs)} (Positive: {len(sampled_positive)}, Negative: {len(sampled_negative)})\")\n",
    "\n",
    "    elif hasattr(dataset, 'triplets'):\n",
    "        # This is a TripletDataset\n",
    "        print(f\"Subsetting a TripletDataset...\")\n",
    "        original_size = len(dataset.triplets)\n",
    "        print(f\"Original dataset - Size: {original_size} triplets\")\n",
    "\n",
    "        # For triplets, we don't need to balance. Just take a random sample.\n",
    "        if original_size > size:\n",
    "            dataset.triplets = random.sample(dataset.triplets, size)\n",
    "        \n",
    "        print(f\"New subset size: {len(dataset.triplets)} triplets\")\n",
    "        \n",
    "    else:\n",
    "        # Fallback if we get an unknown dataset type\n",
    "        print(\"Warning: Unknown dataset type in create_small_subset_for_testing. Skipping subset creation.\")\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356c7172-0039-44ed-ace2-6c38a7c6859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(config, lang_vocab, num_languages):\n",
    "    \"\"\"\n",
    "    Creates the correct dataloaders based on the model_type and test_mode in the config.\n",
    "    \"\"\"\n",
    "    model_type = config['model_type']\n",
    "    batch_size = config['hyperparameters']['batch_size']\n",
    "    test_mode = config.get('test_mode', False)\n",
    "    test_size = config.get('test_size', 10000)\n",
    "\n",
    "    # These are needed for initializing the datasets\n",
    "    DATA_PATH = 's3://md-data-content-recommendation/SNN-training-data/optimized4/'\n",
    "    BUCKET_NAME = 'md-data-content-recommendation'\n",
    "    TEXT_CSV = 's3://md-data-content-recommendation/cleaned_textual_trailers_dataset_with_languages.csv'\n",
    "    ABLATION_MODE = 'full'\n",
    "    \n",
    "    train_loader, val_loader = None, None\n",
    "    positive_pairs_set = None\n",
    "\n",
    "    if model_type == 'pair':\n",
    "        print(\"--- Creating Dataloaders for Pair-based training ---\")\n",
    "        train_dataset = PairDataset(DATA_PATH, BUCKET_NAME, TEXT_CSV, ABLATION_MODE, split='train', lang_vocab=lang_vocab)\n",
    "        val_dataset = PairDataset(DATA_PATH, BUCKET_NAME, TEXT_CSV, ABLATION_MODE, split='validation', lang_vocab=lang_vocab)\n",
    "        \n",
    "        if test_mode:\n",
    "            print(f\"--- Applying Test Mode: Subsetting datasets to size ~{test_size} ---\")\n",
    "            train_dataset = create_small_subset_for_testing(train_dataset, test_size)\n",
    "            val_dataset = create_small_subset_for_testing(val_dataset, test_size // 5)\n",
    "            \n",
    "    elif model_type == 'triplet':\n",
    "        print(\"--- Creating Dataloaders for Online Triplet Mining ---\")\n",
    "        train_dataset = AllMoviesDataset(DATA_PATH, BUCKET_NAME, TEXT_CSV, ABLATION_MODE, split='train', lang_vocab=lang_vocab)\n",
    "        val_dataset = TripletDatasetForValidation(DATA_PATH, BUCKET_NAME, TEXT_CSV, ABLATION_MODE, split='validation', lang_vocab=lang_vocab)\n",
    "        \n",
    "        # We still need the full set of positive pairs for the mining logic in the training loop\n",
    "        pos_pair_helper = TripletDatasetForOnlineTraining(DATA_PATH, BUCKET_NAME, TEXT_CSV, 'full', split='train', lang_vocab=lang_vocab)\n",
    "        positive_pairs_set = {tuple(sorted(p)) for p in pos_pair_helper.anchor_positive_pairs}\n",
    "        del pos_pair_helper\n",
    "        \n",
    "        if test_mode:\n",
    "            print(f\"--- Applying Test Mode: Subsetting datasets to size ~{test_size} ---\")\n",
    "            # For AllMoviesDataset, we sample the list of movie_ids\n",
    "            original_size = len(train_dataset.movie_ids)\n",
    "            train_dataset.movie_ids = random.sample(train_dataset.movie_ids, min(test_size, original_size))\n",
    "            print(f\"Subsetted Triplet training movies from {original_size} to {len(train_dataset.movie_ids)}\")\n",
    "\n",
    "            val_dataset = create_small_subset_for_testing(val_dataset, test_size // 5)\n",
    "\n",
    "    # Create DataLoaders from the (potentially subsetted) datasets\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    return {'train': train_loader, 'val': val_loader}, positive_pairs_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3edd1dd-986c-40b8-aa03-7332dd47c988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gradient_flow(model):\n",
    "    \"\"\"Check if gradients are flowing properly\"\"\"\n",
    "\n",
    "    total_norm_sq = 0.0\n",
    "    param_count = 0\n",
    "    small_grad_count = 0\n",
    "    large_grad_count = 0\n",
    "    nan_grad_count = 0\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and param.grad is not None:\n",
    "            param_norm = param.grad.data.norm(2).item()\n",
    "            param_count += 1\n",
    "\n",
    "            if math.isnan(param_norm) or math.isinf(param_norm):\n",
    "                nan_grad_count += 1\n",
    "                print(f\"   ❌ NAN/INF gradient: {name}\")\n",
    "                continue  # Skip it in total norm computation\n",
    "\n",
    "            total_norm_sq += param_norm ** 2\n",
    "\n",
    "            if param_norm < 1e-7:\n",
    "                small_grad_count += 1\n",
    "                print(f\"   ⚠️  SMALL gradient: {name} = {param_norm:.2e}\")\n",
    "            elif param_norm > 100:\n",
    "                large_grad_count += 1\n",
    "                print(f\"   ⚠️  LARGE gradient: {name} = {param_norm:.2e}\")\n",
    "\n",
    "    if param_count - nan_grad_count > 0:\n",
    "        total_norm = (total_norm_sq) ** 0.5\n",
    "    else:\n",
    "        total_norm = float('nan')\n",
    "\n",
    "    print(f\"🔍 GRADIENT DIAGNOSIS:\")\n",
    "    print(f\"   Total gradient norm: {total_norm:.4f}\")\n",
    "    print(f\"   Parameters with gradients: {param_count}\")\n",
    "    print(f\"   Very small gradients: {small_grad_count}\")\n",
    "    print(f\"   Very large gradients: {large_grad_count}\")\n",
    "    print(f\"   ❌ NaN/Inf gradients: {nan_grad_count}\")\n",
    "\n",
    "    if nan_grad_count > 0: # <<< ADD THIS CONDITION FIRST\n",
    "        print(f\"   ❌ CRITICAL: {nan_grad_count} NaN/Inf gradients detected! Gradient flow is unhealthy.\")\n",
    "    elif math.isnan(total_norm) or math.isinf(total_norm):\n",
    "        print(\"   ❌ NAN or INF in total gradient norm! Likely exploding gradients!\")\n",
    "    elif total_norm < 1e-4:\n",
    "        print(\"   ⚠️  VANISHING GRADIENTS DETECTED!\")\n",
    "    elif total_norm > 1000: # Increased threshold for exploding, as previous explosions were >1000\n",
    "        print(\"   ⚠️  EXPLODING GRADIENTS DETECTED!\")\n",
    "    else:\n",
    "        print(\"   ✅ Gradient flow looks healthy (for non-NaN/Inf parameters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497dde6f-8928-40cf-beae-0f0e18e9496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOT FINAL LEARNNG CURVES\n",
    "def plot_final_learning_curves(log_dir, experiment_name):\n",
    "    \"\"\"\n",
    "    Finds the TensorBoard event file in a specific log directory and plots\n",
    "    the training and validation loss curves, saving it with the experiment name.\n",
    "    \"\"\"\n",
    "    event_file = None\n",
    "    for f in os.listdir(log_dir):\n",
    "        if f.startswith('events.out.tfevents'):\n",
    "            event_file = os.path.join(log_dir, f)\n",
    "            break\n",
    "    \n",
    "    if not event_file:\n",
    "        print(f\"Error: No event file found in {log_dir} for {experiment_name}\")\n",
    "        return\n",
    "\n",
    "    ea = EventAccumulator(event_file).Reload()\n",
    "    \n",
    "    # Extract data\n",
    "    try:\n",
    "        train_loss = [(s.step, s.value) for s in ea.Scalars('Loss/Train')]\n",
    "        val_loss = [(s.step, s.value) for s in ea.Scalars('Loss/Validation')]\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Could not find scalar tag in log for {experiment_name}: {e}\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot([p[0] for p in train_loss], [p[1] for p in train_loss], label='Training Loss', marker='o')\n",
    "    plt.plot([p[0] for p in val_loss], [p[1] for p in val_loss], label='Validation Loss', marker='o')\n",
    "    \n",
    "    # Highlight best model\n",
    "    best_epoch_idx = np.argmin([p[1] for p in val_loss])\n",
    "    best_val_loss = val_loss[best_epoch_idx][1]\n",
    "    best_epoch = val_loss[best_epoch_idx][0]\n",
    "    \n",
    "    plt.axvline(x=best_epoch, color='g', linestyle='--', label=f'Best Model (Epoch {best_epoch+1})')\n",
    "    plt.scatter(best_epoch, best_val_loss, s=100, color='g', zorder=5)\n",
    "\n",
    "    plt.title(f'Learning Curves: {experiment_name}', fontsize=16)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plot_path = Path(log_dir) / f'final_learning_curve_{experiment_name}.png'\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"✅ Final learning curve plot saved to {plot_path}\")\n",
    "    return str(plot_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfa35a2-e6fd-437d-9ce4-e06a4b6fc72e",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96728678-41b0-4649-a59f-963f30694b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_tensorboard(local_log_dir='master_tmp_logs', s3_bucket='md-data-content-recommendation', s3_prefix='n2n-model/logs'):\n",
    "    \"\"\"\n",
    "    Set up TensorBoard with local logs that can be periodically synced to S3.\n",
    "    \n",
    "    Args:\n",
    "        local_log_dir: Local directory to store logs temporarily\n",
    "        s3_bucket: S3 bucket name\n",
    "        s3_prefix: Prefix path in the S3 bucket\n",
    "        \n",
    "    Returns:\n",
    "        SummaryWriter instance and the log directory\n",
    "    \"\"\"\n",
    "    # Create a unique log directory name with timestamp\n",
    "    timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "    log_dir = f\"{local_log_dir}/{timestamp}\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    \n",
    "    # Store S3 information for later syncing\n",
    "    writer.s3_bucket = s3_bucket\n",
    "    writer.s3_prefix = f\"{s3_prefix}/{timestamp}\"\n",
    "    \n",
    "    print(f\"TensorBoard logs will be saved to: {log_dir}\")    \n",
    "    return writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fab007-5dbf-40bf-8281-beb77ea96386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================\n",
    "# UNIFIED PAIR TRAINING AND HELPER FUNCTIONS\n",
    "# ==================================================================\n",
    "\n",
    "def _calculate_and_log_pair_metrics(y_true, distances, loss_type, margin, writer, prefix, epoch):\n",
    "    \"\"\"\n",
    "    Calculates and logs a comprehensive set of metrics for pair-based models.\n",
    "    \"\"\"\n",
    "    if not y_true or not distances:\n",
    "        print(f\"Warning: Empty labels or distances for {prefix}. Skipping metrics.\")\n",
    "        return {}\n",
    "\n",
    "    y_true_np = np.array(y_true)\n",
    "    distances_np = np.array(distances)\n",
    "    \n",
    "    # Invert distance to get a similarity score for ranking metrics\n",
    "    if loss_type == 'cosine':\n",
    "        similarity_scores_np = 1.0 - distances_np\n",
    "    else: # euclidean\n",
    "        similarity_scores_np = np.exp(-distances_np)\n",
    "\n",
    "    # For binary classification metrics, use margin as a threshold\n",
    "    y_pred_np = (distances_np < margin * 0.8).astype(int) # Heuristic threshold\n",
    "\n",
    "    metrics = {}\n",
    "    if len(np.unique(y_true_np)) > 1:\n",
    "        metrics['accuracy'] = accuracy_score(y_true_np, y_pred_np)\n",
    "        metrics['precision'] = precision_score(y_true_np, y_pred_np, zero_division=0)\n",
    "        metrics['recall'] = recall_score(y_true_np, y_pred_np, zero_division=0)\n",
    "        metrics['f1'] = f1_score(y_true_np, y_pred_np, zero_division=0)\n",
    "        try:\n",
    "            metrics['auc_roc'] = roc_auc_score(y_true_np, similarity_scores_np)\n",
    "            metrics['auc_pr'] = average_precision_score(y_true_np, similarity_scores_np)\n",
    "        except ValueError:\n",
    "            metrics['auc_roc'], metrics['auc_pr'] = 0.0, 0.0\n",
    "            \n",
    "        # Log confusion matrix for validation set\n",
    "        if prefix == 'Val':\n",
    "            cm = confusion_matrix(y_true_np, y_pred_np)\n",
    "            fig, ax = plt.subplots(figsize=(6, 5))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, \n",
    "                        xticklabels=['Pred Different', 'Pred Similar'], yticklabels=['True Different', 'True Similar'])\n",
    "            ax.set_title(f'Confusion Matrix - Epoch {epoch+1}')\n",
    "            writer.add_figure('ConfusionMatrix', fig, epoch)\n",
    "            plt.close(fig)\n",
    "\n",
    "    pos_dists = distances_np[y_true_np == 1]\n",
    "    neg_dists = distances_np[y_true_np == 0]\n",
    "    metrics['avg_pos_dist'] = np.mean(pos_dists) if len(pos_dists) > 0 else 0\n",
    "    metrics['avg_neg_dist'] = np.mean(neg_dists) if len(neg_dists) > 0 else 0\n",
    "    metrics['dist_separation'] = metrics['avg_neg_dist'] - metrics['avg_pos_dist']\n",
    "    \n",
    "    # Log to TensorBoard\n",
    "    for name, value in metrics.items():\n",
    "        if isinstance(value, (float, np.number)):\n",
    "            writer.add_scalar(f'Metrics_{prefix}/{name}', value, epoch)\n",
    "            \n",
    "    return metrics\n",
    "\n",
    "def train_pair_model(model, loaders, criterion, optimizer, scheduler, device, config):\n",
    "    \"\"\"\n",
    "    Unified training loop for pair-based models with full diagnostics.\n",
    "    \"\"\"\n",
    "    epochs = config['hyperparameters']['epochs']\n",
    "    s3_prefix = config['checkpoint_s3_prefix']\n",
    "    loss_type = config['loss_type']\n",
    "    start_epoch = config.get('start_epoch', 0)\n",
    "    patience = config.get('patience', 5)\n",
    "    \n",
    "    writer = setup_tensorboard(s3_prefix=f\"n2n-model/logs/{config['run_name']}\")\n",
    "    best_val_loss = float('inf')\n",
    "    no_improve = 0\n",
    "\n",
    "    print(f\"--- Starting Pair-Based Training ({loss_type.capitalize()} Distance) ---\")\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        # --- TRAINING PHASE ---\n",
    "        model.train()\n",
    "        train_loss_sum = 0.0\n",
    "        train_epoch_labels, train_epoch_distances = [], []\n",
    "        \n",
    "        progress_bar_train = tqdm(loaders['train'], desc=f\"Epoch {epoch+1}/{epochs} Training\", leave=False)\n",
    "        for batch in progress_bar_train:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            model_args = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items() if k not in ['movie1_id', 'movie2_id', 'label']}\n",
    "            embedding1, embedding2 = model(**model_args)\n",
    "            loss = criterion(embedding1, embedding2, batch['label'].to(device))\n",
    "            \n",
    "            if torch.isnan(loss):\n",
    "                print(f\"❌ LOSS IS NaN at Epoch {epoch+1}. Skipping batch.\")\n",
    "                continue\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss_sum += loss.item()\n",
    "            progress_bar_train.set_postfix(loss=loss.item())\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                dist_func = (lambda e1, e2: 1 - F.cosine_similarity(e1, e2)) if loss_type == 'cosine' else F.pairwise_distance\n",
    "                distances_batch = dist_func(embedding1, embedding2).cpu().numpy()\n",
    "                train_epoch_labels.extend(batch['label'].cpu().numpy())\n",
    "                train_epoch_distances.extend(distances_batch)\n",
    "\n",
    "        avg_train_loss = train_loss_sum / len(loaders['train'])\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "\n",
    "        # --- VALIDATION PHASE ---\n",
    "        model.eval()\n",
    "        val_loss_sum = 0.0\n",
    "        val_epoch_labels, val_epoch_distances = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(loaders['val'], desc=f\"Epoch {epoch+1}/{epochs} Validation\", leave=False):\n",
    "                model_args = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items() if k not in ['movie1_id', 'movie2_id', 'label']}\n",
    "                embedding1, embedding2 = model(**model_args)\n",
    "                val_loss_sum += criterion(embedding1, embedding2, batch['label'].to(device)).item()\n",
    "                \n",
    "                dist_func = (lambda e1, e2: 1 - F.cosine_similarity(e1, e2)) if loss_type == 'cosine' else F.pairwise_distance\n",
    "                distances_val = dist_func(embedding1, embedding2).cpu().numpy()\n",
    "                val_epoch_labels.extend(batch['label'].cpu().numpy())\n",
    "                val_epoch_distances.extend(distances_val)\n",
    "\n",
    "        avg_val_loss = val_loss_sum / len(loaders['val'])\n",
    "        writer.add_scalar('Loss/Validation', avg_val_loss, epoch)\n",
    "        \n",
    "        # --- METRICS & LOGGING ---\n",
    "        train_metrics = _calculate_and_log_pair_metrics(train_epoch_labels, train_epoch_distances, loss_type, criterion.margin, writer, 'Train', epoch)\n",
    "        val_metrics = _calculate_and_log_pair_metrics(val_epoch_labels, val_epoch_distances, loss_type, criterion.margin, writer, 'Val', epoch)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val F1: {val_metrics.get('f1', 'N/A'):.4f} | Val AUC: {val_metrics.get('auc_roc', 'N/A'):.4f}\")\n",
    "        print(f\"  Distances -> Pos: {val_metrics.get('avg_pos_dist', 'N/A'):.4f} | Neg: {val_metrics.get('avg_neg_dist', 'N/A'):.4f} | Sep: {val_metrics.get('dist_separation', 'N/A'):.4f}\")\n",
    "\n",
    "        # --- CHECKPOINTING & EARLY STOPPING ---\n",
    "        scheduler.step(avg_val_loss)\n",
    "        is_best = avg_val_loss < best_val_loss\n",
    "        if is_best:\n",
    "            best_val_loss = avg_val_loss\n",
    "            no_improve = 0\n",
    "            print(f\"🎉 New best model found at epoch {epoch+1}!\")\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            \n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1, 'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(), 'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_loss': avg_val_loss, 'config': config, 'metrics': val_metrics\n",
    "        }, is_best=is_best, s3_prefix=s3_prefix)\n",
    "        \n",
    "        if no_improve >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}.\")\n",
    "            break\n",
    "            \n",
    "    writer.close()\n",
    "    print(\"--- Pair-Based Training Finished ---\")\n",
    "    return model, writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51be56e3-f240-4d9f-b85d-42250d3e4581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _log_triplet_diagnostics(writer, pos_dists, neg_dists, margin, loss_type, epoch, prefix):\n",
    "    \"\"\"Logs detailed validation metrics for triplet models to TensorBoard.\"\"\"\n",
    "    if not pos_dists or not neg_dists:\n",
    "        return\n",
    "        \n",
    "    avg_pos_dist = np.mean(pos_dists)\n",
    "    avg_neg_dist = np.mean(neg_dists)\n",
    "    separation = avg_neg_dist - avg_pos_dist\n",
    "    \n",
    "    writer.add_scalar(f'Metrics_{prefix}/Avg_Positive_Dist_{loss_type}', avg_pos_dist, epoch)\n",
    "    writer.add_scalar(f'Metrics_{prefix}/Avg_Negative_Dist_{loss_type}', avg_neg_dist, epoch)\n",
    "    writer.add_scalar(f'Metrics_{prefix}/Dist_Separation_{loss_type}', separation, epoch)\n",
    "\n",
    "    # Calculate percentage of \"satisfied\" triplets (where loss would be 0)\n",
    "    satisfied_triplets = np.mean(np.array(pos_dists) - np.array(neg_dists) + margin < 0)\n",
    "    writer.add_scalar(f'Metrics_{prefix}/Satisfied_Triplets_Percent', satisfied_triplets * 100, epoch)\n",
    "    \n",
    "    print(f\"{prefix} Metrics -> PosDist: {avg_pos_dist:.4f} | NegDist: {avg_neg_dist:.4f} | Sep: {separation:.4f} | Satisfied: {satisfied_triplets:.2%}\")\n",
    "    \n",
    "    if prefix == 'Val':\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        sns.histplot(pos_dists, color=\"green\", label=f'Anchor-Positive (Mean: {avg_pos_dist:.2f})', kde=True, stat=\"density\", element=\"step\", ax=ax)\n",
    "        sns.histplot(neg_dists, color=\"red\", label=f'Anchor-Negative (Mean: {avg_neg_dist:.2f})', kde=True, stat=\"density\", element=\"step\", ax=ax)\n",
    "        plt.axvline(x=margin, color='blue', linestyle='--', linewidth=2, label=f'Margin ({margin})')\n",
    "        plt.title(f'Validation Distance Distribution - Epoch {epoch+1}')\n",
    "        plt.legend()\n",
    "        writer.add_figure('DistanceDistribution', fig, epoch)\n",
    "        plt.close(fig)\n",
    "\n",
    "def train_triplet_model(model, train_loader, val_loader, positive_pairs_set, criterion, optimizer, scheduler, device, config):\n",
    "    \"\"\"\n",
    "    Unified training loop for triplet-based models with semi-hard mining.\n",
    "    \"\"\"\n",
    "    epochs = config['hyperparameters']['epochs']\n",
    "    s3_prefix = config['checkpoint_s3_prefix']\n",
    "    loss_type = config['loss_type']\n",
    "    start_epoch = config.get('start_epoch', 0)\n",
    "    patience = config.get('patience', 5)\n",
    "\n",
    "    writer = setup_tensorboard(s3_prefix=f\"n2n-model/logs/{config['run_name']}\")\n",
    "    best_val_loss = float('inf')\n",
    "    no_improve = 0\n",
    "\n",
    "    print(f\"--- Starting Triplet Training ({loss_type.capitalize()} Distance) ---\")\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        # --- TRAINING PHASE ---\n",
    "        model.train()\n",
    "        train_loss_sum = 0.0\n",
    "        \n",
    "        progress_bar_train = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} Training\", leave=False)\n",
    "        for i, batch in enumerate(progress_bar_train):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch_data = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "            embeddings = model(**batch_data)\n",
    "            movie_ids = batch_data['movie_id']\n",
    "            \n",
    "            # --- In-Batch Semi-Hard Negative Mining ---\n",
    "            distance_matrix = torch.cdist(embeddings, embeddings) if loss_type == 'euclidean' else (1 - torch.matmul(embeddings, embeddings.t()))\n",
    "            \n",
    "            anchors, positives, negatives = [], [], []\n",
    "            for j in range(len(movie_ids)):\n",
    "                anchor_id, anchor_emb = movie_ids[j].item(), embeddings[j]\n",
    "                \n",
    "                # Find positives in the batch\n",
    "                pos_indices = [k for k, pos_id in enumerate(movie_ids) if j != k and tuple(sorted((anchor_id, pos_id.item()))) in positive_pairs_set]\n",
    "                if not pos_indices: continue\n",
    "                \n",
    "                # Select one positive\n",
    "                pos_idx = pos_indices[0]\n",
    "                positive_emb = embeddings[pos_idx]\n",
    "                \n",
    "                dist_ap = distance_matrix[j, pos_idx]\n",
    "                \n",
    "                # Find semi-hard negatives\n",
    "                neg_mask = torch.ones(len(movie_ids), dtype=torch.bool, device=device)\n",
    "                neg_mask[j] = False\n",
    "                neg_mask[pos_indices] = False\n",
    "                \n",
    "                dist_an = distance_matrix[j, neg_mask]\n",
    "                semi_hard_mask = (dist_an > dist_ap) & (dist_an < dist_ap + criterion.margin)\n",
    "                \n",
    "                if torch.any(semi_hard_mask):\n",
    "                    semi_hard_indices = torch.where(neg_mask)[0][semi_hard_mask]\n",
    "                    chosen_neg_idx = random.choice(semi_hard_indices)\n",
    "                    negative_emb = embeddings[chosen_neg_idx]\n",
    "                    \n",
    "                    anchors.append(anchor_emb)\n",
    "                    positives.append(positive_emb)\n",
    "                    negatives.append(negative_emb)\n",
    "\n",
    "            if not anchors: continue\n",
    "            \n",
    "            loss = criterion(torch.stack(anchors), torch.stack(positives), torch.stack(negatives))\n",
    "            if torch.isnan(loss): continue\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
    "            optimizer.step()\n",
    "            train_loss_sum += loss.item()\n",
    "            progress_bar_train.set_postfix(loss=loss.item(), triplets=len(anchors))\n",
    "\n",
    "        avg_train_loss = train_loss_sum / len(train_loader)\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "\n",
    "        # --- VALIDATION PHASE ---\n",
    "        model.eval()\n",
    "        val_loss_sum = 0.0\n",
    "        val_pos_dists, val_neg_dists = [], []\n",
    "        with torch.no_grad():\n",
    "            progress_bar_val = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} Validation\", leave=False)\n",
    "            for batch in progress_bar_val:\n",
    "                # anchor_emb = model(**{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch['anchor_data'].items()})\n",
    "                # positive_emb = model(**{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch['positive_data'].items()})\n",
    "                # negative_emb = model(**{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch['negative_data'].items()})\n",
    "\n",
    "                # Process each part of the triplet separately to save memory\n",
    "                anchor_data = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch['anchor_data'].items()}\n",
    "                anchor_emb = model(**anchor_data)\n",
    "                \n",
    "                positive_data = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch['positive_data'].items()}\n",
    "                positive_emb = model(**positive_data)\n",
    "                \n",
    "                negative_data = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch['negative_data'].items()}\n",
    "                negative_emb = model(**negative_data)\n",
    "                \n",
    "                val_loss_sum += criterion(anchor_emb, positive_emb, negative_emb).item()\n",
    "                \n",
    "                dist_func = (lambda e1, e2: 1 - F.cosine_similarity(e1, e2)) if loss_type == 'cosine' else F.pairwise_distance\n",
    "                val_pos_dists.extend(dist_func(anchor_emb, positive_emb).cpu().numpy())\n",
    "                val_neg_dists.extend(dist_func(anchor_emb, negative_emb).cpu().numpy())\n",
    "                \n",
    "        avg_val_loss = val_loss_sum / len(val_loader)\n",
    "        writer.add_scalar('Loss/Validation', avg_val_loss, epoch)\n",
    "\n",
    "        # --- METRICS & LOGGING ---\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "        _log_triplet_diagnostics(writer, val_pos_dists, val_neg_dists, criterion.margin, loss_type, epoch, 'Val')\n",
    "        if i % 100 == 1: check_gradient_flow(model) # Check gradients periodically\n",
    "\n",
    "        # --- CHECKPOINTING & EARLY STOPPING ---\n",
    "        scheduler.step(avg_val_loss)\n",
    "        is_best = avg_val_loss < best_val_loss\n",
    "        if is_best:\n",
    "            best_val_loss = avg_val_loss\n",
    "            no_improve = 0\n",
    "            print(f\"🎉 New best model found at epoch {epoch+1}!\")\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            \n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1, 'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(), 'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_loss': avg_val_loss, 'config': config\n",
    "        }, is_best=is_best, s3_prefix=s3_prefix)\n",
    "        \n",
    "        if no_improve >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}.\")\n",
    "            break\n",
    "            \n",
    "    writer.close()\n",
    "    print(\"--- Triplet-Based Training Finished ---\")\n",
    "    return model, writer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e9b2a5-9b6e-4695-b957-878bd89a6a9d",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708c55f8-ac38-4b5a-8700-c3f6afa21cd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the TensorBoard extension and launch it\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir master_tmp_logs --port 6001 --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a128a43-cdc7-4b10-b519-64ef3f30f2cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # --- Global Constants ---\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    BUCKET_NAME = 'md-data-content-recommendation'\n",
    "    TEXT_CSV = 's3://md-data-content-recommendation/cleaned_textual_trailers_dataset_with_languages.csv'\n",
    "    ABLATION_MODE = 'full'\n",
    "    NUM_FRAMES_FROM_PT, DINO_DIM, VGGISH_DIM = 16, 384, 128\n",
    "    SAMPLE_RATE, MAX_AUDIO_LENGTH_SEC = 16000, 10\n",
    "    DINO_PER_FRAME_EMBEDDING_S3_PREFIX = 'movie_trailers_dino_per_frame_embeddings/'\n",
    "    AUDIO_VGGISH_WAVE_S3_PREFIX = 'movie_trailers_audio_embeddings_vggish/'\n",
    "\n",
    "    # === THE CONTROL PANEL FOR ALL EXPERIMENTS ===\n",
    "    experiments_to_run = {\n",
    "        \"E2E_Pair_Cosine\": {\n",
    "            \"model_type\": \"pair\",\n",
    "            \"loss_type\": \"cosine\",\n",
    "            \"fusion_network\": \"Hybrid\",\n",
    "            \"checkpoint_s3_prefix\": \"n2n-model/25-cosine-with-languages-checkpoints/\",\n",
    "            \"hyperparameters\": {\"lr\": 5e-5, \"batch_size\": 16, \"epochs\": 50},\n",
    "            \"patience\": 8, # increase patience\n",
    "            \"test_mode\": True,      \n",
    "            \"test_size\": 25000 \n",
    "        },\n",
    "        \"E2E_Pair_Euclidean\": {\n",
    "            \"model_type\": \"pair\",\n",
    "            \"loss_type\": \"euclidean\",\n",
    "            \"fusion_network\": \"Simple\",\n",
    "            \"checkpoint_s3_prefix\": \"n2n-model/25-euclidean-with-languages-checkpoints/\",\n",
    "            \"hyperparameters\": {\"lr\":5e-5, \"batch_size\": 16, \"epochs\": 50},\n",
    "            \"patience\": 8, # increase patience\n",
    "            \"test_mode\": True,\n",
    "            \"test_size\": 25000\n",
    "        },\n",
    "        \"E2E_Triplet_Cosine\": {\n",
    "            \"model_type\": \"triplet\",\n",
    "            \"loss_type\": \"cosine\",\n",
    "            \"fusion_network\": \"Simple\",\n",
    "            \"checkpoint_s3_prefix\": \"n2n-model/25-SEMI-HARD-triplet-cosine-checkpoints/\",\n",
    "            \"hyperparameters\": {\"lr\": 1e-4, \"batch_size\": 16, \"epochs\": 100},\n",
    "            \"patience\": 10, # increase patience\n",
    "            \"test_mode\": True,\n",
    "            \"test_size\": 25000\n",
    "        },\n",
    "        \"E2E_Triplet_Euclidean\": {\n",
    "            \"model_type\": \"triplet\",\n",
    "            \"loss_type\": \"euclidean\",\n",
    "            \"fusion_network\": \"UltraSimple\",\n",
    "            \"checkpoint_s3_prefix\": \"n2n-model/25-SEMI-HARD-triplet-euclidean-checkpoints/\", # normal = 2e-4, 2- = 1e-4, patience = 10\n",
    "            \"hyperparameters\": {\"lr\": 1e-4, \"batch_size\": 16, \"epochs\": 100},\n",
    "            \"patience\": 10, # increase patience\n",
    "            \"test_mode\": True,\n",
    "            \"test_size\": 25000\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # --- Pre-build Vocabulary ONCE ---\n",
    "    master_text_dataset = TextDataset(TEXT_CSV, lang_vocab=None)\n",
    "    num_languages = master_text_dataset.num_languages\n",
    "    shared_lang_vocab = master_text_dataset.lang_vocab\n",
    "\n",
    "    # To store the final results\n",
    "    all_experiment_results = {}\n",
    "\n",
    "    # --- MASTER LOOP ---\n",
    "    for name, config in experiments_to_run.items():\n",
    "        print(f\"\\n{'='*30}\\nSTARTING EXPERIMENT: {name}\\n{'='*30}\")\n",
    "        config['run_name'] = name # Add a name for logging\n",
    "        set_seed(42) # Reset seed for each experiment for reproducibility\n",
    "\n",
    "        # 1. Create Dataloaders for this experiment\n",
    "        loaders, positive_pairs_set = create_dataloaders(config, shared_lang_vocab, num_languages)\n",
    "        \n",
    "        # 2. Create Model and Criterion\n",
    "        model, criterion, optimizer, scheduler = create_model_and_criterion(config, DEVICE, num_languages)\n",
    "\n",
    "        final_writer = None # Initialize writer\n",
    "        \n",
    "        # 3. Call the correct, specific training loop\n",
    "        if config['model_type'] == 'pair':\n",
    "            print(\">>> Dispatching to PAIR training loop...\")\n",
    "            train_pair_model(\n",
    "                model, loaders, criterion, optimizer, scheduler, DEVICE, config\n",
    "            )\n",
    "        elif config['model_type'] == 'triplet':\n",
    "            print(\">>> Dispatching to TRIPLET training loop...\")\n",
    "            train_triplet_model(\n",
    "                model, loaders['train'], loaders['val'], positive_pairs_set,\n",
    "                criterion, optimizer, scheduler, DEVICE, config\n",
    "            )\n",
    "\n",
    "        # 4. Generate final plots\n",
    "        if final_writer:\n",
    "            print(\"\\nFlushing and closing TensorBoard writer...\")\n",
    "            final_writer.flush()\n",
    "            final_writer.close()\n",
    "            time.sleep(2) # Give filesystem a moment\n",
    "            plot_final_learning_curves(final_writer.log_dir, name)\n",
    "            \n",
    "        print(f\"\\n{'='*30}\\nFINISHED EXPERIMENT: {name}\\n{'='*30}\")\n",
    "        \n",
    "    print(\"\\nAll experiments have been completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf1bafc-479e-468e-b83d-d7373c2d042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip -r master_logs.zip master_tmp_logs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
