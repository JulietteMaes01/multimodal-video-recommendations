{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35eb0d4f-e0c1-4f5f-b46a-ed085e08b6ff",
   "metadata": {},
   "source": [
    "# SIamese Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c1cc5a2-b30e-4c4e-9cce-3cbd0db8a3cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "from itertools import combinations\n",
    "from typing import List, Tuple, Dict, Set # Ensure Set is imported\n",
    "import os\n",
    "import pickle # Ensure pickle is imported for saving\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SiameseDataGenerator:\n",
    "    def __init__(self, bucket_name: str, ratings_path: str, metadata_path: str,\n",
    "                 frames_prefix: str, output_s3_path_prefix: str,\n",
    "                 min_pairs_per_movie: int = 5, random_seed: int = 42): \n",
    "        self.bucket_name = bucket_name\n",
    "        self.ratings_path = ratings_path\n",
    "        self.metadata_path = metadata_path\n",
    "        self.frames_prefix = frames_prefix\n",
    "        self.output_s3_path_prefix = output_s3_path_prefix\n",
    "        self.min_pairs_per_movie = min_pairs_per_movie\n",
    "        random.seed(random_seed) # For reproducibility\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "        self.s3_client = boto3.client('s3')\n",
    "        self.ratings_df = None\n",
    "        self.metadata_df = None\n",
    "        self.available_movies = set()\n",
    "        self.user_ratings = defaultdict(dict)\n",
    "        self.movie_genres = {}\n",
    "        self.positive_pairs = []\n",
    "        self.negative_pairs = []\n",
    "        self.triplets = [] \n",
    "\n",
    "    def load_data(self):\n",
    "        print(\"Loading data from S3...\")\n",
    "        obj = self.s3_client.get_object(Bucket=self.bucket_name, Key=self.ratings_path.replace(f\"s3://{self.bucket_name}/\", \"\"))\n",
    "        self.ratings_df = pd.read_csv(obj['Body'])\n",
    "        print(f\"Loaded {len(self.ratings_df)} ratings\")\n",
    "\n",
    "        obj = self.s3_client.get_object(Bucket=self.bucket_name, Key=self.metadata_path.replace(f\"s3://{self.bucket_name}/\", \"\"))\n",
    "        self.metadata_df = pd.read_csv(obj['Body'])\n",
    "        if 'movieId' in self.metadata_df.columns:\n",
    "            self.metadata_df.set_index('movieId', inplace=True, drop=False) \n",
    "        print(f\"Loaded metadata for {len(self.metadata_df)} movies\")\n",
    "\n",
    "        self._get_available_movies()\n",
    "        self._filter_data()\n",
    "        self._prepare_data_structures()\n",
    "\n",
    "    def _get_available_movies(self):\n",
    "        print(\"Getting available movies from S3 frame tensors...\")\n",
    "        paginator = self.s3_client.get_paginator('list_objects_v2')\n",
    "        s3_prefix_path = self.frames_prefix.replace(f\"s3://{self.bucket_name}/\", \"\")\n",
    "        \n",
    "        movie_ids = set()\n",
    "        print(f\"Scanning S3 prefix: {s3_prefix_path}\")\n",
    "        for page in tqdm(paginator.paginate(Bucket=self.bucket_name, Prefix=s3_prefix_path), desc=\"S3 pages for frames\"):\n",
    "            if 'Contents' in page:\n",
    "                for item in page['Contents']:\n",
    "                    key = item['Key']\n",
    "                    if key.endswith('_frames.pt'):\n",
    "                        try:\n",
    "                            movie_id_str = os.path.basename(key).split('_frames.pt')[0]\n",
    "                            if movie_id_str.isdigit():\n",
    "                                movie_ids.add(int(movie_id_str))\n",
    "                        except:\n",
    "                            pass # Ignore malformed filenames\n",
    "        self.available_movies = movie_ids\n",
    "        print(f\"Found {len(self.available_movies)} available movies with frame tensors.\")\n",
    "\n",
    "\n",
    "    def _filter_data(self):\n",
    "        print(\"Filtering data based on available movies...\")\n",
    "        initial_ratings = len(self.ratings_df)\n",
    "        self.ratings_df = self.ratings_df[self.ratings_df['movieId'].isin(self.available_movies)].copy()\n",
    "        print(f\"Filtered ratings: {initial_ratings} -> {len(self.ratings_df)}\")\n",
    "\n",
    "        initial_metadata = len(self.metadata_df)\n",
    "        self.metadata_df = self.metadata_df[self.metadata_df['movieId'].isin(self.available_movies)].copy()\n",
    "        print(f\"Filtered metadata: {initial_metadata} -> {len(self.metadata_df)}\")\n",
    "\n",
    "        movies_with_ratings = set(self.ratings_df['movieId'].unique())\n",
    "        movies_with_metadata = set(self.metadata_df['movieId'].unique())\n",
    "        \n",
    "        # Final available_movies are those present in frames, ratings, and metadata\n",
    "        self.available_movies = self.available_movies.intersection(movies_with_ratings, movies_with_metadata)\n",
    "        print(f\"Final available movies (in frames, ratings, metadata): {len(self.available_movies)}\")\n",
    "        \n",
    "        # Further filter DataFrames to ensure consistency\n",
    "        self.ratings_df = self.ratings_df[self.ratings_df['movieId'].isin(self.available_movies)].copy()\n",
    "        self.metadata_df = self.metadata_df[self.metadata_df['movieId'].isin(self.available_movies)].copy()\n",
    "\n",
    "\n",
    "    def _prepare_data_structures(self):\n",
    "        print(\"Preparing data structures...\")\n",
    "        for _, row in tqdm(self.ratings_df.iterrows(), total=len(self.ratings_df), desc=\"Processing ratings\"):\n",
    "            user_id = int(row['userId'])\n",
    "            movie_id = int(row['movieId'])\n",
    "            if movie_id not in self.available_movies:\n",
    "                continue\n",
    "            liked = row['rating'] >= 4.0 # Consistent threshold\n",
    "            self.user_ratings[user_id][movie_id] = liked\n",
    "\n",
    "        for movie_id, row in tqdm(self.metadata_df.iterrows(), total=len(self.metadata_df), desc=\"Processing genres\"):\n",
    "            if movie_id not in self.available_movies: \n",
    "                continue\n",
    "            genres_str = row.get('genres', '')\n",
    "            self.movie_genres[movie_id] = set(genres_str.split('|')) if pd.notna(genres_str) and genres_str else set()\n",
    "        print(f\"Prepared data for {len(self.user_ratings)} users and {len(self.movie_genres)} movies.\")\n",
    "\n",
    "    def _calculate_genre_jaccard(self, movie1_id: int, movie2_id: int) -> float:\n",
    "        genres1 = self.movie_genres.get(movie1_id, set())\n",
    "        genres2 = self.movie_genres.get(movie2_id, set())\n",
    "        if not genres1 and not genres2:\n",
    "            return 0.0 \n",
    "        intersection_size = len(genres1.intersection(genres2))\n",
    "        union_size = len(genres1.union(genres2))\n",
    "        if union_size == 0:\n",
    "            return 0.0\n",
    "        return intersection_size / union_size\n",
    "\n",
    "    def generate_positive_pairs(self, min_genre_jaccard_for_positive: float = 0.25):\n",
    "        print(f\"Generating positive pairs (min Jaccard: {min_genre_jaccard_for_positive})...\")\n",
    "        positive_pairs_set = set()\n",
    "        movie_pair_count = Counter()\n",
    "\n",
    "        for user_id, ratings in tqdm(self.user_ratings.items(), desc=\"Users for positive pairs\"):\n",
    "            liked_movies = [mid for mid, liked in ratings.items() if liked and mid in self.movie_genres]\n",
    "            if len(liked_movies) < 2:\n",
    "                continue\n",
    "            for m1, m2 in combinations(liked_movies, 2):\n",
    "                if self._calculate_genre_jaccard(m1, m2) >= min_genre_jaccard_for_positive:\n",
    "                    pair = tuple(sorted((m1, m2)))\n",
    "                    positive_pairs_set.add(pair)\n",
    "                    movie_pair_count[m1] += 1\n",
    "                    movie_pair_count[m2] += 1\n",
    "        \n",
    "        self.positive_pairs = list(positive_pairs_set)\n",
    "        print(f\"Generated {len(self.positive_pairs)} positive pairs.\")\n",
    "        return movie_pair_count\n",
    "\n",
    "    def generate_negative_pairs(self, target_count: int = None,\n",
    "                                max_genre_jaccard_for_negative: float = 0.1,\n",
    "                                min_user_disagreement_score: int = 1):\n",
    "        print(f\"Generating negative pairs (max Jaccard: {max_genre_jaccard_for_negative}, min disagreement: {min_user_disagreement_score})...\")\n",
    "        if target_count is None:\n",
    "            target_count = len(self.positive_pairs)\n",
    "        \n",
    "        positive_pairs_set = set(self.positive_pairs)\n",
    "        candidate_negatives_scores = defaultdict(int)\n",
    "\n",
    "        # Strategy:\n",
    "        # 1. User-based evidence: Find pairs where users disagreed (liked one, disliked other)\n",
    "        #    AND these pairs have low genre similarity.\n",
    "        print(\"Scoring negatives based on user disagreement & low genre Jaccard...\")\n",
    "        for user_id, ratings in tqdm(self.user_ratings.items(), desc=\"Users for negative candidates\"):\n",
    "            rated_movie_ids = list(m_id for m_id in ratings.keys() if m_id in self.movie_genres) # Ensure movie has genres\n",
    "            if len(rated_movie_ids) < 2:\n",
    "                continue\n",
    "            for m1, m2 in combinations(rated_movie_ids, 2):\n",
    "                pair = tuple(sorted((m1, m2)))\n",
    "                if pair in positive_pairs_set:\n",
    "                    continue\n",
    "\n",
    "                genre_jaccard = self._calculate_genre_jaccard(m1, m2)\n",
    "                if genre_jaccard > max_genre_jaccard_for_negative:\n",
    "                    continue # Too similar in genre to be a good negative\n",
    "\n",
    "                m1_liked = ratings[m1]\n",
    "                m2_liked = ratings[m2]\n",
    "\n",
    "                if (m1_liked and not m2_liked) or (not m1_liked and m2_liked):\n",
    "                    candidate_negatives_scores[pair] += 2 # Strong disagreement\n",
    "                elif not m1_liked and not m2_liked: # Both disliked\n",
    "                    candidate_negatives_scores[pair] += 1 # Weaker negative evidence\n",
    "\n",
    "        # Filter by min_user_disagreement_score and prepare for sampling\n",
    "        scored_potential_negatives = [\n",
    "            (pair, score) for pair, score in candidate_negatives_scores.items()\n",
    "            if score >= min_user_disagreement_score\n",
    "        ]\n",
    "        print(f\"Found {len(scored_potential_negatives)} candidates from user disagreement strategy.\")\n",
    "\n",
    "        # Sort by score (descending) then shuffle to break ties for sampling\n",
    "        random.shuffle(scored_potential_negatives)\n",
    "        scored_potential_negatives.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        self.negative_pairs = [pair for pair, score in scored_potential_negatives]\n",
    "        \n",
    "        # Supplement if not enough pairs\n",
    "        num_needed_supplement = target_count - len(self.negative_pairs)\n",
    "        if num_needed_supplement > 0:\n",
    "            print(f\"Need to supplement {num_needed_supplement} more negative pairs.\")\n",
    "            all_movie_ids_list = list(self.available_movies)\n",
    "            current_neg_set = set(self.negative_pairs)\n",
    "            supplement_attempts = 0\n",
    "            max_supplement_attempts = num_needed_supplement * 200 # Limit attempts\n",
    "\n",
    "            pbar_supplement = tqdm(total=num_needed_supplement, desc=\"Supplementing random negatives\")\n",
    "            while len(self.negative_pairs) < target_count and supplement_attempts < max_supplement_attempts:\n",
    "                supplement_attempts += 1\n",
    "                m1, m2 = random.sample(all_movie_ids_list, 2)\n",
    "                pair = tuple(sorted((m1, m2)))\n",
    "\n",
    "                if pair not in positive_pairs_set and pair not in current_neg_set:\n",
    "                    if self._calculate_genre_jaccard(m1, m2) <= max_genre_jaccard_for_negative:\n",
    "                        self.negative_pairs.append(pair)\n",
    "                        current_neg_set.add(pair)\n",
    "                        pbar_supplement.update(1)\n",
    "            pbar_supplement.close()\n",
    "\n",
    "        # If still over target, trim. If under, that's the max we could get.\n",
    "        if len(self.negative_pairs) > target_count:\n",
    "            self.negative_pairs = random.sample(self.negative_pairs, target_count)\n",
    "        \n",
    "        movie_pair_count = Counter()\n",
    "        for pair in self.negative_pairs:\n",
    "            movie_pair_count[pair[0]] += 1\n",
    "            movie_pair_count[pair[1]] += 1\n",
    "            \n",
    "        print(f\"Generated {len(self.negative_pairs)} negative pairs in total.\")\n",
    "        return movie_pair_count\n",
    "\n",
    "    def generate_triplets(self, num_neg_per_anchor_positive=1):\n",
    "        print(f\"Generating triplets ({num_neg_per_anchor_positive} negatives per anchor-positive)...\")\n",
    "        self.triplets = []\n",
    "        \n",
    "        # Create a quick lookup for movie genres\n",
    "        # And a list of all movies for random negative sampling\n",
    "        all_movies_with_genres = list(mid for mid in self.available_movies if mid in self.movie_genres)\n",
    "\n",
    "        for anchor_id, positive_id in tqdm(self.positive_pairs, desc=\"Generating triplets\"):\n",
    "            if anchor_id not in self.movie_genres or positive_id not in self.movie_genres:\n",
    "                continue\n",
    "\n",
    "            negatives_found_for_this_ap = 0\n",
    "            # Try to find \"harder\" negatives: different from anchor but might share *some* characteristic with anchor, but definitely different from positive based on genre.\n",
    "            # This is a simplified hard negative mining.\n",
    "            \n",
    "            shuffled_candidates = random.sample(all_movies_with_genres, len(all_movies_with_genres))\n",
    "\n",
    "            for negative_candidate_id in shuffled_candidates:\n",
    "                if negatives_found_for_this_ap >= num_neg_per_anchor_positive:\n",
    "                    break\n",
    "                \n",
    "                if negative_candidate_id == anchor_id or negative_candidate_id == positive_id:\n",
    "                    continue\n",
    "\n",
    "                # Condition for a \"good\" negative in a triplet (A, P, N):\n",
    "                # 1. N is not P.\n",
    "                # 2. N is contextually different from P (e.g., low genre similarity with P).\n",
    "                # 3. (Optional, for harder negatives) N might share some similarity with A, but less than P does.\n",
    "                # For simplicity here, we'll focus on N being different from P by genre.\n",
    "                \n",
    "                # Anchor and Positive are similar (by definition of positive_pairs)\n",
    "                # Negative should be dissimilar to the Anchor (and implicitly Positive)\n",
    "                # A stricter condition might be _calculate_genre_jaccard(anchor_id, negative_candidate_id) < threshold\n",
    "                # AND _calculate_genre_jaccard(positive_id, negative_candidate_id) < threshold\n",
    "\n",
    "                # Simpler: Ensure negative is quite different from the positive one based on genre\n",
    "                if self._calculate_genre_jaccard(positive_id, negative_candidate_id) <= 0.1: # N very different from P\n",
    "                    # And (optionally) ensure N is not *too* similar to A either\n",
    "                    if self._calculate_genre_jaccard(anchor_id, negative_candidate_id) <= 0.5: # N not overly similar to A\n",
    "                        self.triplets.append((anchor_id, positive_id, negative_candidate_id))\n",
    "                        negatives_found_for_this_ap += 1\n",
    "                        \n",
    "        print(f\"Generated {len(self.triplets)} triplets.\")\n",
    "\n",
    "\n",
    "    def ensure_minimum_pairs_per_movie(self, positive_counts: Counter, negative_counts: Counter):\n",
    "        print(f\"Ensuring each movie appears in at least {self.min_pairs_per_movie} pairs...\")\n",
    "        \n",
    "        all_movies_in_pairs = set(positive_counts.keys()).union(set(negative_counts.keys()))\n",
    "        movies_list = list(self.available_movies) # All movies that have frames, ratings, metadata\n",
    "\n",
    "        for movie_id in tqdm(movies_list, desc=\"Checking min pairs per movie\"):\n",
    "            current_pos_count = positive_counts[movie_id]\n",
    "            current_neg_count = negative_counts[movie_id]\n",
    "            total_current_pairs = current_pos_count + current_neg_count\n",
    "            \n",
    "            needed = self.min_pairs_per_movie - total_current_pairs\n",
    "            if needed <= 0:\n",
    "                continue\n",
    "\n",
    "            added_count = 0\n",
    "            random.shuffle(movies_list) # Shuffle to get varied partners\n",
    "\n",
    "            for other_movie_id in movies_list:\n",
    "                if added_count >= needed: break\n",
    "                if movie_id == other_movie_id: continue\n",
    "                \n",
    "                pair = tuple(sorted((movie_id, other_movie_id)))\n",
    "                is_positive_candidate = self._calculate_genre_jaccard(movie_id, other_movie_id) >= 0.25 # Threshold from generate_positive_pairs\n",
    "                \n",
    "                user_co_liked = False\n",
    "                for user_ratings_dict in self.user_ratings.values():\n",
    "                    if user_ratings_dict.get(movie_id, False) and user_ratings_dict.get(other_movie_id, False):\n",
    "                        user_co_liked = True\n",
    "                        break\n",
    "                \n",
    "                if is_positive_candidate and user_co_liked and pair not in self.positive_pairs:\n",
    "                    self.positive_pairs.append(pair)\n",
    "                    positive_counts[movie_id] += 1\n",
    "                    positive_counts[other_movie_id] += 1\n",
    "                    added_count +=1\n",
    "\n",
    "            for other_movie_id in movies_list:\n",
    "                if added_count >= needed: break\n",
    "                if movie_id == other_movie_id: continue\n",
    "\n",
    "                pair = tuple(sorted((movie_id, other_movie_id)))\n",
    "                is_negative_candidate = self._calculate_genre_jaccard(movie_id, other_movie_id) <= 0.1 # Threshold from generate_negative_pairs\n",
    "\n",
    "                if is_negative_candidate and pair not in self.positive_pairs and pair not in self.negative_pairs:\n",
    "                    self.negative_pairs.append(pair)\n",
    "                    negative_counts[movie_id] += 1\n",
    "                    negative_counts[other_movie_id] += 1\n",
    "                    added_count += 1\n",
    "            \n",
    "        print(f\"Final counts after ensuring min pairs - Positive: {len(self.positive_pairs)}, Negative: {len(self.negative_pairs)}\")\n",
    "\n",
    "\n",
    "    def create_train_eval_test_splits(self, train_ratio=0.7, eval_ratio=0.15):\n",
    "        print(\"Creating train/eval/test splits...\")\n",
    "        test_ratio = 1.0 - train_ratio - eval_ratio\n",
    "        assert abs(train_ratio + eval_ratio + test_ratio - 1.0) < 1e-6, \"Ratios must sum to 1\"\n",
    "\n",
    "        all_labeled_pairs = [(pair, 1) for pair in self.positive_pairs] + \\\n",
    "                            [(pair, 0) for pair in self.negative_pairs]\n",
    "        random.shuffle(all_labeled_pairs)\n",
    "\n",
    "        total_pairs = len(all_labeled_pairs)\n",
    "        train_end_idx = int(total_pairs * train_ratio)\n",
    "        eval_end_idx = train_end_idx + int(total_pairs * eval_ratio)\n",
    "\n",
    "        train_data = all_labeled_pairs[:train_end_idx]\n",
    "        eval_data = all_labeled_pairs[train_end_idx:eval_end_idx]\n",
    "        test_data = all_labeled_pairs[eval_end_idx:]\n",
    "\n",
    "        print(f\"Split sizes - Train: {len(train_data)}, Eval: {len(eval_data)}, Test: {len(test_data)}\")\n",
    "        return {'train': train_data, 'validation': eval_data, 'test': test_data} # Use 'validation' to match PairDataset\n",
    "\n",
    "\n",
    "    def get_movie_info(self, movie_id: int) -> str:\n",
    "        try:\n",
    "            if movie_id in self.metadata_df.index:\n",
    "                row = self.metadata_df.loc[movie_id]\n",
    "                title = row.get('title', 'Unknown Title')\n",
    "                year_val = row.get('year', 'N/A')\n",
    "                # Ensure year is treated as a string, handling potential float if NaN was present then filled\n",
    "                year_str = str(int(year_val)) if pd.notna(year_val) and isinstance(year_val, (float, int)) else str(year_val)\n",
    "                genres = row.get('genres', 'Unknown Genres')\n",
    "                return f\"{title} ({year_str}) - {genres}\"\n",
    "            else:\n",
    "                return f\"Movie ID {movie_id} - Metadata not found\"\n",
    "        except KeyError:\n",
    "            return f\"Movie ID {movie_id} - Metadata lookup error\"\n",
    "        except Exception as e:\n",
    "            return f\"Movie ID {movie_id} - Error getting info: {e}\"\n",
    "\n",
    "    def show_samples(self, num_samples=5):\n",
    "        print(f\"\\n=== SAMPLE DATA ({num_samples} samples) ===\")\n",
    "        if self.positive_pairs:\n",
    "            print(f\"\\nSample Positive Pairs (movieId1, movieId2):\")\n",
    "            for pair in random.sample(self.positive_pairs, min(num_samples, len(self.positive_pairs))):\n",
    "                print(f\"  ({pair[0]}, {pair[1]})\")\n",
    "                print(f\"    M1: {self.get_movie_info(pair[0])}\")\n",
    "                print(f\"    M2: {self.get_movie_info(pair[1])}\")\n",
    "        if self.negative_pairs:\n",
    "            print(f\"\\nSample Negative Pairs (movieId1, movieId2):\")\n",
    "            for pair in random.sample(self.negative_pairs, min(num_samples, len(self.negative_pairs))):\n",
    "                print(f\"  ({pair[0]}, {pair[1]})\")\n",
    "                print(f\"    M1: {self.get_movie_info(pair[0])}\")\n",
    "                print(f\"    M2: {self.get_movie_info(pair[1])}\")\n",
    "        if self.triplets:\n",
    "            print(f\"\\nSample Triplets (anchor, positive, negative):\")\n",
    "            for triplet in random.sample(self.triplets, min(num_samples, len(self.triplets))):\n",
    "                print(f\"  ({triplet[0]}, {triplet[1]}, {triplet[2]})\")\n",
    "                print(f\"    Anchor:   {self.get_movie_info(triplet[0])}\")\n",
    "                print(f\"    Positive: {self.get_movie_info(triplet[1])}\")\n",
    "                print(f\"    Negative: {self.get_movie_info(triplet[2])}\")\n",
    "\n",
    "    def calculate_statistics(self):\n",
    "        print(f\"\\n=== STATISTICS ===\")\n",
    "        movie_pair_counts = Counter()\n",
    "        all_unique_pairs = set(self.positive_pairs).union(set(self.negative_pairs))\n",
    "        for pair in all_unique_pairs:\n",
    "            movie_pair_counts[pair[0]] += 1\n",
    "            movie_pair_counts[pair[1]] += 1\n",
    "        \n",
    "        pair_counts_values = list(movie_pair_counts.values())\n",
    "        avg_pairs = np.mean(pair_counts_values) if pair_counts_values else 0\n",
    "        min_pairs = min(pair_counts_values) if pair_counts_values else 0\n",
    "        max_pairs = max(pair_counts_values) if pair_counts_values else 0\n",
    "\n",
    "        print(f\"Total unique movies in generated pairs: {len(movie_pair_counts)}\")\n",
    "        print(f\"Total positive pairs: {len(self.positive_pairs)}\")\n",
    "        print(f\"Total negative pairs: {len(self.negative_pairs)}\")\n",
    "        print(f\"Total unique pairs generated: {len(all_unique_pairs)}\")\n",
    "        print(f\"Total triplets: {len(self.triplets)}\")\n",
    "        print(f\"Average pairs per movie involved: {avg_pairs:.2f}\")\n",
    "        print(f\"Min pairs for an involved movie: {min_pairs}\")\n",
    "        print(f\"Max pairs for an involved movie: {max_pairs}\")\n",
    "        \n",
    "        movies_with_min_pairs = sum(1 for count in pair_counts_values if count >= self.min_pairs_per_movie)\n",
    "        print(f\"Movies involved in at least {self.min_pairs_per_movie} pairs: {movies_with_min_pairs}/{len(movie_pair_counts)}\")\n",
    "\n",
    "    def _save_splits_to_pkl(self, splits_data_with_labels):\n",
    "        print(\"Saving pair splits to S3 as pickle files...\")\n",
    "        # Correctly parse S3 prefix from output_s3_path_prefix\n",
    "        if self.output_s3_path_prefix.startswith(\"s3://\"):\n",
    "            path_parts = self.output_s3_path_prefix.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "            bucket = path_parts[0]\n",
    "            prefix = path_parts[1] if len(path_parts) > 1 else \"\"\n",
    "        else: # Assume it's just a prefix if s3:// is missing (for local testing)\n",
    "            bucket = self.bucket_name \n",
    "            prefix = self.output_s3_path_prefix\n",
    "\n",
    "        if prefix and not prefix.endswith('/'):\n",
    "            prefix += '/'\n",
    "\n",
    "        for split_name, pairs_with_labels_list in splits_data_with_labels.items():\n",
    "            positive_split_pairs = [pair_tuple for pair_tuple, label in pairs_with_labels_list if label == 1]\n",
    "            negative_split_pairs = [pair_tuple for pair_tuple, label in pairs_with_labels_list if label == 0]\n",
    "\n",
    "            for pairs_list_to_save, type_suffix in [\n",
    "                (positive_split_pairs, \"positive_pairs\"), \n",
    "                (negative_split_pairs, \"negative_pairs\")\n",
    "            ]:\n",
    "                if not pairs_list_to_save:\n",
    "                    print(f\"No {type_suffix} to save for {split_name} split.\")\n",
    "                    continue\n",
    "\n",
    "                s3_key = f\"{prefix}{split_name}_{type_suffix}.pkl\"\n",
    "                try:\n",
    "                    pickle_byte_obj = pickle.dumps(pairs_list_to_save)\n",
    "                    self.s3_client.put_object(Bucket=bucket, Key=s3_key, Body=pickle_byte_obj)\n",
    "                    print(f\"Saved {len(pairs_list_to_save)} {type_suffix} for {split_name} to s3://{bucket}/{s3_key}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error saving {split_name}_{type_suffix}.pkl to S3 for bucket {bucket}, key {s3_key}: {e}\")\n",
    "                    \n",
    "    def _save_triplets_to_pkl(self, triplets_data_list_of_tuples):\n",
    "        if not triplets_data_list_of_tuples:\n",
    "            print(\"No triplets to save.\")\n",
    "            return\n",
    "        print(\"Saving triplets to S3 as pickle file...\")\n",
    "        if self.output_s3_path_prefix.startswith(\"s3://\"):\n",
    "            path_parts = self.output_s3_path_prefix.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "            bucket = path_parts[0]\n",
    "            prefix = path_parts[1] if len(path_parts) > 1 else \"\"\n",
    "        else:\n",
    "            bucket = self.bucket_name\n",
    "            prefix = self.output_s3_path_prefix\n",
    "            \n",
    "        if prefix and not prefix.endswith('/'):\n",
    "            prefix += '/'\n",
    "        \n",
    "        s3_key = f\"{prefix}all_triplets.pkl\" \n",
    "        try:\n",
    "            pickle_byte_obj = pickle.dumps(triplets_data_list_of_tuples)\n",
    "            self.s3_client.put_object(Bucket=bucket, Key=s3_key, Body=pickle_byte_obj)\n",
    "            print(f\"Saved {len(triplets_data_list_of_tuples)} triplets to s3://{bucket}/{s3_key}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving triplets.pkl to S3 for bucket {bucket}, key {s3_key}: {e}\")\n",
    "\n",
    "    def run_full_pipeline(self):\n",
    "        print(\"Starting Siamese Network Data Generation Pipeline\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        self.load_data()\n",
    "        \n",
    "        pos_counts = self.generate_positive_pairs(min_genre_jaccard_for_positive=0.25)\n",
    "        neg_counts = self.generate_negative_pairs(\n",
    "            target_count=len(self.positive_pairs),\n",
    "            max_genre_jaccard_for_negative=0.1, # Stricter: Max 10% genre overlap for negatives\n",
    "            min_user_disagreement_score=1       # Require at least some user disagreement\n",
    "        )\n",
    "        \n",
    "        self.ensure_minimum_pairs_per_movie(pos_counts, neg_counts) \n",
    "        \n",
    "        # Re-balance if ensure_minimum_pairs skewed counts significantly\n",
    "        if len(self.positive_pairs) != len(self.negative_pairs):\n",
    "            print(\"Re-balancing positive and negative pairs after ensuring minimums...\")\n",
    "            if len(self.positive_pairs) > len(self.negative_pairs):\n",
    "                self.positive_pairs = random.sample(self.positive_pairs, len(self.negative_pairs))\n",
    "            else:\n",
    "                self.negative_pairs = random.sample(self.negative_pairs, len(self.positive_pairs))\n",
    "            print(f\"Re-balanced to Positive: {len(self.positive_pairs)}, Negative: {len(self.negative_pairs)}\")\n",
    "\n",
    "        self.generate_triplets(num_neg_per_anchor_positive=1)\n",
    "        \n",
    "        splits_data = self.create_train_eval_test_splits()\n",
    "        \n",
    "        self.show_samples()\n",
    "        self.calculate_statistics()\n",
    "        \n",
    "        self._save_splits_to_pkl(splits_data)\n",
    "        self._save_triplets_to_pkl(self.triplets) \n",
    "\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"Data generation pipeline completed successfully!\")\n",
    "        return splits_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    BUCKET_NAME = 'md-data-content-recommendation'\n",
    "    OUTPUT_S3_PATH_PREFIX = \"SNN-training-data/optimized4\"\n",
    "    \n",
    "    MOVIELENS_RATINGS_KEY = \"ratings_filtered.csv\" \n",
    "    MOVIE_METADATA_KEY = \"cleaned_textual_trailers_dataset.csv\" \n",
    "    FRAMES_S3_PREFIX_FOR_SCANNING = \"movie_trailers_frames_tensors/\" \n",
    "    \n",
    "    generator = SiameseDataGenerator(\n",
    "        bucket_name=BUCKET_NAME,\n",
    "        ratings_path=f\"s3://{BUCKET_NAME}/{MOVIELENS_RATINGS_KEY}\",\n",
    "        metadata_path=f\"s3://{BUCKET_NAME}/{MOVIE_METADATA_KEY}\",  \n",
    "        frames_prefix=FRAMES_S3_PREFIX_FOR_SCANNING,\n",
    "        output_s3_path_prefix=OUTPUT_S3_PATH_PREFIX, \n",
    "        min_pairs_per_movie=5, e\n",
    "        random_seed=42\n",
    "    )\n",
    "    \n",
    "    splits_data = generator.run_full_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
