# multimodal-video-recommendations
Master Thesis project on building a deep intermodal fusion learning network for content-based video recommendations.

# "Seeing, Hearing, and Reading": A Deep Intermodal Fusion Learning Network for Content-Based Video Recommendations

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/release/python-3100/)

This repository contains the source code and experimental artifacts for the Master Thesis by Juliette Maes, submitted to Maastricht University, Department of Advanced Computer Science. The research focuses on developing and evaluating a purely content-based recommender system for movies, leveraging visual, audio, and textual modalities to address the cold-start problem.

---

### ðŸ“„ Thesis

The full Master Thesis PDF containing detailed background, methodology, and analysis can be found here: [[LINK TO THESIS PDF]][(https://we.tl/t-GqS5GVLtMU)](https://github.com/JulietteMaes01/multimodal-video-recommendations/blob/main/Master_Thesis_Juliette_Maes.pdf).

---

### ðŸ“‚ Repository Structure & Code Overview

This repository is provided as a **research artifact** to accompany the thesis. The code is organized to provide both a structural overview of the final models and a direct, executable trace of the research process.

*   **`src/`**: Contains the final, refactored Python modules for all key components of the project (data loaders, model architectures, loss functions, etc.). This directory provides the cleanest, most readable view of the code logic described in the thesis.
*   **`notebooks/`**: Contains the primary Jupyter notebooks used for data generation, training, and evaluation. These notebooks serve as the main, executable record of the research and are the recommended starting point for understanding the end-to-end workflow.
---

### ðŸš€ Getting Started & Reproducing Results

**Prerequisites:**
*   Python 3.10+
*   [Poetry](https://python-poetry.org/) for dependency management
*   An AWS account with credentials configured, as the dataset and models are stored in S3.
*   An NVIDIA GPU is required for training.

**1. Installation**
Clone the repository and install the required dependencies using Poetry.
```bash
git clone https://github.com/your-username/multimodal-video-recommendations.git
cd multimodal-video-recommendations
```

**2. Walkthrough using Jupyter Notebooks**
The most direct way to understand and reproduce the results is to follow the Jupyter notebooks in the `notebooks/` directory in numerical order.

*   **`01_data_generation.ipynb`**: This notebook contains the complete logic for generating the positive/negative pairs and triplets used for training the Siamese networks.
*   **`02_master_trainer.ipynb`**: This is the master control panel for training all four end-to-end models from the 2x2 experimental design. You can select which experiment to run within the notebook.
*   **`03_evaluation_and_visualization.ipynb`**: This notebook loads the recommendation files generated by the trained models and calculates the final performance metrics (Precision@K, nDCG@10, etc.) presented in the thesis.

---

### ðŸ’¡ Note on Code Structure

The code in this repository is presented in two formats:
1.  **Refactored `.py` modules in the `src/` directory** provide a clean, structural representation of the project's components.
2.  **Complete `.ipynb` files in the `notebooks/` directory** provide an executable, step-by-step log of the research process.

To transition this research code into a fully installable command-line package, the modules in `src/` would require the addition of `argparse` for command-line arguments and the corresponding `import` statements. For the purposes of thesis verification, the notebooks provide the most direct and transparent view of the work.

---

### ðŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
